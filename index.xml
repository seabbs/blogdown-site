<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sam Abbott on Sam Abbott</title>
    <link>http://www.samabbott.co.uk/</link>
    <description>Recent content in Sam Abbott on Sam Abbott</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>&amp;copy; 2018 Sam Abbott</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Benchmarking an Rstats workstation on realistic workloads - using xgboost via h2o</title>
      <link>http://www.samabbott.co.uk/post/benchmarking-workstation-xgboost/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/benchmarking-workstation-xgboost/</guid>
      <description>


&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;I recently &lt;a href=&#34;https://www.samabbott.co.uk/post/building-an-rstats-workstation/&#34;&gt;built out a new workstation&lt;/a&gt; to give me some local compute for data science workloads. Now that I have local access to both a CPU with a large number of cores (Threadripper 1950X with 16 cores) and a moderately powerful GPU (Nvidia RTX 2070), I’m interested in knowing when it is best to use CPU vs. GPU for some of the tasks that I commonly do.&lt;/p&gt;
&lt;p&gt;The first of these is fitting &lt;code&gt;xgboost&lt;/code&gt; models for prediction. This makes sense as a first problem to explore as in my experience, and in the experience of the wider community, &lt;code&gt;xgboost&lt;/code&gt; generally provides the best performance on tabular data - light GBM looks like it may be even better but the installation appears to be nightmarish - and predictive modelling is a fairly common use case. As I have recently been using the &lt;a href=&#34;https://www.h2o.ai&#34;&gt;&lt;code&gt;h2o&lt;/code&gt; package&lt;/a&gt; as my go-to tool, it makes sense for me to test &lt;code&gt;xgboost&lt;/code&gt; via &lt;code&gt;h2o&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I am also interested in exploring whether or not simultaneous multithreading (i.e Hyper-threading for Intel CPUs) gives any performance boost over using only physical cores for these workloads. I couldn’t find much on this online for AMD CPUs. My prior experience with Intel CPUs is that sticking to physical cores is the best option for nearly all serious compute. If this proves to be the case, disabling virtual core gives me a greater scope for overclocking!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;developing-a-testing-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Developing a testing function&lt;/h2&gt;
&lt;p&gt;To make this a relatively real-world test, I am going to be comparing run times on a grid of cross-validated models (10 models, with 5 folds each). A nice benefit of this is that we can also see the average performance of a configuration across a variety of hyper-parameters. In the code below I have specified the grid and used the &lt;code&gt;purrr:partial&lt;/code&gt; function to wrap everything up into a function. I’ve also turned off early stopping, which is not something that I would do in a real use case, to allow control over the exact number of trees being trained.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Search criteria
search_crit &amp;lt;- list(strategy = &amp;quot;RandomDiscrete&amp;quot;, 
                    max_models = 10, stopping_rounds = 10)

hyper_params &amp;lt;- list(
  learn_rate = c(0.1, 0.3, 0.5),
  sample_rate = c(0.6, 0.8, 1),
  col_sample_rate = c(0.6, 0.8, 1),
  max_depth = c(1, 5, 10),
  min_rows = c(1, 2, 5),
  reg_lambda = c(0, 0.001),
  reg_alpha = c(0, 0.001)
)

spec_grid &amp;lt;- partial(h2o.grid,
                     algorithm = &amp;quot;xgboost&amp;quot;,
                     nfolds = 5,
                     seed = 2344232,
                     stopping_rounds = 0,
                     search_criteria = search_crit,
                     hyper_params = hyper_params)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to develop a function to fit and time a single grid. This needs to be specified by a subsample of the rows and columns, on a given number of CPU cores, and potentially with a GPU backend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmark_grid &amp;lt;- function(df,
                           target,
                           grid = NULL,
                           cores = NULL, 
                           gpu = FALSE,
                           rowsample = 1e3,
                           trees = NULL,
                           colsample = 1,
                           ram = 28) {
  
## Initialise the h2o cluster with the desired core number
  h2o.init(min_mem_size = paste0(ram, &amp;quot;g&amp;quot;),
           nthreads = cores)
  h2o.no_progress()

## Sample columns (up/down sampling)
  df &amp;lt;- df[target] %&amp;gt;% 
    bind_cols(df %&amp;gt;%
                select(-contains(target)) %&amp;gt;% 
                {.[, sample(1:ncol(.), colsample, replace = TRUE)]})
  
## Specify the training data and set 
  h2o_train &amp;lt;- sample_n(df, rowsample, replace = TRUE) %&amp;gt;% 
    as.h2o

## Specify the features
  features &amp;lt;- setdiff(colnames(df), target)
  
## Start the timer
tic(paste0(&amp;quot;Trained a &amp;quot;,
           &amp;quot;grid of 10 Xgboost &amp;quot;, 
           &amp;quot;models with &amp;quot;, cores, &amp;quot; cores&amp;quot;, 
            ifelse(gpu, &amp;quot; using the GPU backend&amp;quot;, &amp;quot;&amp;quot;),
            &amp;quot; on a subsample of &amp;quot;,
            rowsample, 
            &amp;quot; rows and &amp;quot;,
            colsample, 
           &amp;quot; features with &amp;quot;,
           trees, 
           &amp;quot; trees.&amp;quot;))

  if(object.size(df) &amp;gt; ((ram * 1000^3)/ cores)) {
    message(&amp;quot;Data size is to big to fit into RAM in this configuration&amp;quot;)
    
    model_fit &amp;lt;- FALSE
  }else{
    ## Train the models
  trained_grid &amp;lt;- grid(y = target,
                       x = features,
                       training_frame = h2o_train,
                       backend = ifelse(gpu, &amp;quot;gpu&amp;quot;, &amp;quot;cpu&amp;quot;)) 
  
  model_fit &amp;lt;- TRUE
  }


  
time &amp;lt;- toc()

time$fit &amp;lt;- model_fit

h2o.shutdown(prompt = FALSE)

Sys.sleep(3)
return(time)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sourcing-test-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sourcing test data&lt;/h2&gt;
&lt;p&gt;As the base for my testing data, I am using credit data from the &lt;code&gt;recipes&lt;/code&gt; package as an example of a real-world dataset. I went with a binary outcome as this reflects much of the modelling I have been doing day to day - usually loan defaults.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;credit_data &amp;lt;- recipes::credit_data %&amp;gt;% 
  as_tibble

skim(credit_data) %&amp;gt;% 
  skimr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Skim summary statistics&lt;br /&gt;
n obs: 4454&lt;br /&gt;
n variables: 14&lt;/p&gt;
&lt;p&gt;Variable type: factor&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;variable&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;missing&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;complete&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;n_unique&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;top_counts&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;ordered&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Home&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4448&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;own: 2107, ren: 973, par: 783, oth: 319&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Job&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4452&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;fix: 2805, fre: 1024, par: 452, oth: 171&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Marital&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4453&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;mar: 3241, sin: 977, sep: 130, wid: 67&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Records&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;no: 3681, yes: 773, NA: 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Status&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;goo: 3200, bad: 1254, NA: 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Variable type: integer&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;variable&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;missing&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;complete&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p0&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p25&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p50&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p75&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;p100&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Age&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;37.08&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10.98&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;45&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;68&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▅▇▇▇▅▃▂▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Amount&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1038.92&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;474.55&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;700&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1300&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▅▇▃▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Assets&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4407&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5403.98&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11574.42&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3e+05&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▁▁▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Debt&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4436&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;343.03&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1245.99&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;30000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▁▁▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Expenses&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55.57&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19.52&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;180&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▃▃▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Income&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;381&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4073&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;141.69&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80.75&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;125&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;170&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;959&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▆▁▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Price&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1462.78&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;628.13&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;105&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1117.25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1691.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11140&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▆▁▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Seniority&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7.99&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;8.17&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▇▃▂▁▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Time&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4454&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46.44&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14.66&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;60&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;▁▁▂▃▁▃▇▁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This dataset has been cleaned and contains a limited number of, presumably fairly predictive, variables. To make this a more realistic test I’ve introduced additional numeric and categorical noise variables, as well as adding missing data and duplicating the original features - code below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add numeric and categorical noise features. Categorical features are randomly sampled and assigned 10, 50, 100, 250, 500 and 1000 levels, whilst numeric features are normally distributed with or without a log transform.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Set up categorical variable generation
get_cat_noise_var &amp;lt;- function(levels = NULL, samples) {
  sample(paste(&amp;#39;level&amp;#39;,1:levels,sep=&amp;#39;&amp;#39;), samples, replace=TRUE)
}

## Generate categorical variable with differing lengths (10, 100, 1000)
cat_noise_var &amp;lt;- map(c(10, 50, 100, 250, 500, 1000), ~ rep(., 5)) %&amp;gt;% 
  flatten %&amp;gt;% 
  map_dfc(~get_cat_noise_var(., nrow(credit_data))) %&amp;gt;% 
  set_names(paste0(&amp;quot;CatNoise_&amp;quot;, 1:30)) %&amp;gt;% 
  map_dfc(factor)

## Set up numeric variable generation. Normal with random mean and standard deviation (or log normal)
get_num_noise_var &amp;lt;- function(noise = 0.1, samples, log_shift = FALSE) {
  mean &amp;lt;- runif(1, -1e3, 1e3)
  x &amp;lt;- rnorm(samples, mean, abs(mean) * noise)
  
  if (log_shift) { 
   x &amp;lt;- log(abs(x + 1))
  }
  
  return(x)
}

## Generate numeric variables with varying amounts of noise and transforming using log
gen_numeric_var &amp;lt;- function(df, log_shift) {
  map(c(0.1, 0.2, 0.4), ~ rep(., 5)) %&amp;gt;% 
  flatten %&amp;gt;% 
  map_dfc( ~ get_num_noise_var(., nrow(df), log_shift))
}

num_noise_var &amp;lt;- gen_numeric_var(credit_data, log_shift = FALSE) %&amp;gt;% 
  bind_cols(gen_numeric_var(credit_data, log_shift = TRUE)) %&amp;gt;% 
    set_names(paste0(&amp;quot;NumNoise_&amp;quot;, 1:30))
  

## Bind together and summarise
noise_var &amp;lt;- cat_noise_var %&amp;gt;% 
  bind_cols(num_noise_var)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Add duplicate informative features.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;credit_data &amp;lt;- credit_data %&amp;gt;% 
  select(Status) %&amp;gt;% 
  bind_cols(credit_data %&amp;gt;% 
              select(-Status) %&amp;gt;% 
              {bind_cols(., .)} %&amp;gt;% 
              {bind_cols(., .)})&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Add some missingness to the data and replace the homeowners &lt;code&gt;&amp;quot;other&amp;quot;&lt;/code&gt; category with 1000 random levels. Adding random noise levels to the homeowners variable means that some information is now encoded in a very noisy feature, providing more of a challenge for the &lt;code&gt;xgboost&lt;/code&gt; model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;add_miss &amp;lt;- function(x = NULL, max_miss = NULL) {
  miss_scale &amp;lt;- runif(1, 0, max_miss)
  
  x &amp;lt;- replace(x, runif(length(x), 0, 1) &amp;lt;= miss_scale, NA)
}



complex_credit_data &amp;lt;- credit_data %&amp;gt;% 
  bind_cols(noise_var) %&amp;gt;% 
  mutate_at(.vars = vars(everything(), - Status), ~ add_miss(., 0.2)) %&amp;gt;% 
  mutate(
    Home = case_when(Home %in% &amp;quot;other&amp;quot; ~ as.character(CatNoise_30),
                          TRUE ~ as.character(Home)) %&amp;gt;% 
           factor)


complex_credit_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4,454 x 113
##    Status Seniority Home   Time   Age Marital Records Job   Expenses Income
##    &amp;lt;fct&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;    &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;
##  1 good           9 rent     60    30 &amp;lt;NA&amp;gt;    no      free…       73    129
##  2 good          17 rent     60    58 widow   no      &amp;lt;NA&amp;gt;        48    131
##  3 bad           10 owner    36    46 married &amp;lt;NA&amp;gt;    free…       90    200
##  4 good           0 rent     60    24 single  no      fixed       63    182
##  5 good           0 rent     36    26 single  no      &amp;lt;NA&amp;gt;        46    107
##  6 good           1 owner    60    36 married no      &amp;lt;NA&amp;gt;        NA    214
##  7 good          29 owner    60    44 married no      fixed       75    125
##  8 good           9 pare…    12    27 &amp;lt;NA&amp;gt;    no      fixed       35     80
##  9 good           0 &amp;lt;NA&amp;gt;     60    32 married no      free…       90     NA
## 10 bad            0 pare…    48    41 married no      part…       90     80
## # … with 4,444 more rows, and 103 more variables: Assets &amp;lt;int&amp;gt;,
## #   Debt &amp;lt;int&amp;gt;, Amount &amp;lt;int&amp;gt;, Price &amp;lt;int&amp;gt;, Seniority1 &amp;lt;int&amp;gt;, Home1 &amp;lt;fct&amp;gt;,
## #   Time1 &amp;lt;int&amp;gt;, Age1 &amp;lt;int&amp;gt;, Marital1 &amp;lt;fct&amp;gt;, Records1 &amp;lt;fct&amp;gt;, Job1 &amp;lt;fct&amp;gt;,
## #   Expenses1 &amp;lt;int&amp;gt;, Income1 &amp;lt;int&amp;gt;, Assets1 &amp;lt;int&amp;gt;, Debt1 &amp;lt;int&amp;gt;,
## #   Amount1 &amp;lt;int&amp;gt;, Price1 &amp;lt;int&amp;gt;, Seniority2 &amp;lt;int&amp;gt;, Home2 &amp;lt;fct&amp;gt;,
## #   Time2 &amp;lt;int&amp;gt;, Age2 &amp;lt;int&amp;gt;, Marital2 &amp;lt;fct&amp;gt;, Records2 &amp;lt;fct&amp;gt;, Job2 &amp;lt;fct&amp;gt;,
## #   Expenses2 &amp;lt;int&amp;gt;, Income2 &amp;lt;int&amp;gt;, Assets2 &amp;lt;int&amp;gt;, Debt2 &amp;lt;int&amp;gt;,
## #   Amount2 &amp;lt;int&amp;gt;, Price2 &amp;lt;int&amp;gt;, Seniority11 &amp;lt;int&amp;gt;, Home11 &amp;lt;fct&amp;gt;,
## #   Time11 &amp;lt;int&amp;gt;, Age11 &amp;lt;int&amp;gt;, Marital11 &amp;lt;fct&amp;gt;, Records11 &amp;lt;fct&amp;gt;,
## #   Job11 &amp;lt;fct&amp;gt;, Expenses11 &amp;lt;int&amp;gt;, Income11 &amp;lt;int&amp;gt;, Assets11 &amp;lt;int&amp;gt;,
## #   Debt11 &amp;lt;int&amp;gt;, Amount11 &amp;lt;int&amp;gt;, Price11 &amp;lt;int&amp;gt;, CatNoise_1 &amp;lt;fct&amp;gt;,
## #   CatNoise_2 &amp;lt;fct&amp;gt;, CatNoise_3 &amp;lt;fct&amp;gt;, CatNoise_4 &amp;lt;fct&amp;gt;,
## #   CatNoise_5 &amp;lt;fct&amp;gt;, CatNoise_6 &amp;lt;fct&amp;gt;, CatNoise_7 &amp;lt;fct&amp;gt;,
## #   CatNoise_8 &amp;lt;fct&amp;gt;, CatNoise_9 &amp;lt;fct&amp;gt;, CatNoise_10 &amp;lt;fct&amp;gt;,
## #   CatNoise_11 &amp;lt;fct&amp;gt;, CatNoise_12 &amp;lt;fct&amp;gt;, CatNoise_13 &amp;lt;fct&amp;gt;,
## #   CatNoise_14 &amp;lt;fct&amp;gt;, CatNoise_15 &amp;lt;fct&amp;gt;, CatNoise_16 &amp;lt;fct&amp;gt;,
## #   CatNoise_17 &amp;lt;fct&amp;gt;, CatNoise_18 &amp;lt;fct&amp;gt;, CatNoise_19 &amp;lt;fct&amp;gt;,
## #   CatNoise_20 &amp;lt;fct&amp;gt;, CatNoise_21 &amp;lt;fct&amp;gt;, CatNoise_22 &amp;lt;fct&amp;gt;,
## #   CatNoise_23 &amp;lt;fct&amp;gt;, CatNoise_24 &amp;lt;fct&amp;gt;, CatNoise_25 &amp;lt;fct&amp;gt;,
## #   CatNoise_26 &amp;lt;fct&amp;gt;, CatNoise_27 &amp;lt;fct&amp;gt;, CatNoise_28 &amp;lt;fct&amp;gt;,
## #   CatNoise_29 &amp;lt;fct&amp;gt;, CatNoise_30 &amp;lt;fct&amp;gt;, NumNoise_1 &amp;lt;dbl&amp;gt;,
## #   NumNoise_2 &amp;lt;dbl&amp;gt;, NumNoise_3 &amp;lt;dbl&amp;gt;, NumNoise_4 &amp;lt;dbl&amp;gt;,
## #   NumNoise_5 &amp;lt;dbl&amp;gt;, NumNoise_6 &amp;lt;dbl&amp;gt;, NumNoise_7 &amp;lt;dbl&amp;gt;,
## #   NumNoise_8 &amp;lt;dbl&amp;gt;, NumNoise_9 &amp;lt;dbl&amp;gt;, NumNoise_10 &amp;lt;dbl&amp;gt;,
## #   NumNoise_11 &amp;lt;dbl&amp;gt;, NumNoise_12 &amp;lt;dbl&amp;gt;, NumNoise_13 &amp;lt;dbl&amp;gt;,
## #   NumNoise_14 &amp;lt;dbl&amp;gt;, NumNoise_15 &amp;lt;dbl&amp;gt;, NumNoise_16 &amp;lt;dbl&amp;gt;,
## #   NumNoise_17 &amp;lt;dbl&amp;gt;, NumNoise_18 &amp;lt;dbl&amp;gt;, NumNoise_19 &amp;lt;dbl&amp;gt;,
## #   NumNoise_20 &amp;lt;dbl&amp;gt;, NumNoise_21 &amp;lt;dbl&amp;gt;, NumNoise_22 &amp;lt;dbl&amp;gt;,
## #   NumNoise_23 &amp;lt;dbl&amp;gt;, NumNoise_24 &amp;lt;dbl&amp;gt;, NumNoise_25 &amp;lt;dbl&amp;gt;,
## #   NumNoise_26 &amp;lt;dbl&amp;gt;, NumNoise_27 &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-on-for-a-single-iteration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing on for a single iteration&lt;/h2&gt;
&lt;p&gt;To check that everything is working as expected we test on a single iteration with 31 cores, no GPU, 1000 samples, 20 features and 50 trees.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_test &amp;lt;- benchmark_grid(complex_credit_data,
                            &amp;quot;Status&amp;quot;,
                            grid = spec_grid,
                            cores = 31, 
                            gpu = FALSE,
                            rowsample = 1e4,
                            trees = 50,
                            colsample = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /tmp/RtmppdeaNe/h2o_seabbs_started_from_r.out
##     /tmp/RtmppdeaNe/h2o_seabbs_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: . Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 seconds 213 milliseconds 
##     H2O cluster timezone:       Etc/UTC 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.23.0.4558 
##     H2O cluster version age:    5 days  
##     H2O cluster name:           H2O_started_from_R_seabbs_kha403 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   26.83 GB 
##     H2O cluster total cores:    32 
##     H2O cluster allowed cores:  31 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20) 
## 
## Trained a grid of 10 Xgboost models with 31 cores on a subsample of 10000 rows and 20 features with 50 trees.: 61.636 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $tic
## elapsed 
##   6.104 
## 
## $toc
## elapsed 
##   67.74 
## 
## $msg
## [1] &amp;quot;Trained a grid of 10 Xgboost models with 31 cores on a subsample of 10000 rows and 20 features with 50 trees.&amp;quot;
## 
## $fit
## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the settings above give a runtime of around a minute but using the &lt;code&gt;htop&lt;/code&gt; tool we see that resource use is not stable over time. This may indicate that &lt;code&gt;h2o&lt;/code&gt; is not using all the supplied cores effectively/efficiently for this data size, with these settings etc.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/2019-01-20-benchmarking-workstation-xgboost/load-example.gif&#34; alt=&#34;Load according to htop whilst running the test grid.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Load according to &lt;code&gt;htop&lt;/code&gt; whilst running the test grid.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;enabling-gpu-support&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Enabling GPU support&lt;/h2&gt;
&lt;p&gt;Unlike using CPUs for &lt;code&gt;xgboost&lt;/code&gt;, enabling GPU support requires some extra steps (and lots of faff). As I have a Nvidia GPU, I need to install CUDA on my local machine (see &lt;a href=&#34;https://www.samabbott.co.uk/post/building-an-rstats-workstation/&#34;&gt;here&lt;/a&gt; for details); CUDA 8.0 (or higher) into the Docker container that this analysis is running in (see here for &lt;a href=&#34;https://github.com/seabbs/tidyverse-gpu&#34;&gt;the Dockerfile&lt;/a&gt; - thanks to &lt;a href=&#34;https://discuss.ropensci.org/t/tips-for-installing-cuda-into-a-rocker-docker-container/1556&#34;&gt;Noam Ross&lt;/a&gt; for the original implementation); and run the Docker container using the &lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;Nvidia Docker runtime&lt;/a&gt;. To check everything is working, we run the same benchmark as above but now using the GPU.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_test &amp;lt;- benchmark_grid(complex_credit_data,
                            &amp;quot;Status&amp;quot;,
                            grid = spec_grid,
                            cores = 31, 
                            gpu = TRUE,
                            rowsample = 1e4,
                            trees = 50,
                            colsample = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /tmp/RtmppdeaNe/h2o_seabbs_started_from_r.out
##     /tmp/RtmppdeaNe/h2o_seabbs_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: . Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 seconds 86 milliseconds 
##     H2O cluster timezone:       Etc/UTC 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.23.0.4558 
##     H2O cluster version age:    5 days  
##     H2O cluster name:           H2O_started_from_R_seabbs_pon293 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   26.83 GB 
##     H2O cluster total cores:    32 
##     H2O cluster allowed cores:  31 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20) 
## 
## Trained a grid of 10 Xgboost models with 31 cores using the GPU backend on a subsample of 10000 rows and 20 features with 50 trees.: 236.803 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $tic
## elapsed 
##  73.641 
## 
## $toc
## elapsed 
## 310.444 
## 
## $msg
## [1] &amp;quot;Trained a grid of 10 Xgboost models with 31 cores using the GPU backend on a subsample of 10000 rows and 20 features with 50 trees.&amp;quot;
## 
## $fit
## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Success! However, it has a much longer run time of over 2 minutes - not good. We again see (this time using the &lt;a href=&#34;https://github.com/Syllo/nvtop&#34;&gt;&lt;code&gt;nvtop&lt;/code&gt;&lt;/a&gt; tool) that resource use varies over time on the GPU.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/2019-01-20-benchmarking-workstation-xgboost/load-example-gpu.gif&#34; alt=&#34;Load according to htop whilst running the test grid.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Load according to &lt;code&gt;htop&lt;/code&gt; whilst running the test grid.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;iterating-across-a-grid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Iterating Across a Grid&lt;/h2&gt;
&lt;p&gt;Now that the timing function and the data are in place and everything is tested, I can now run a full benchmarking grid. Using &lt;code&gt;expand.grid&lt;/code&gt;, I’ve combined all combinations of data sizes from 1,000 to 100,000 rows, from 10 to 1000 columns, from 10 to 10,000 trees and compute availability (here 4, 16, and 32 cores + GPU). Something that I have not implemented here, but that would reduce the noise in the final results, is running each benchmark multiple times. As you will see below, this is not feasible for a weekend blog post (or even the week or two blog post that this finally became!). &lt;em&gt;Note: I ended up dropping the 1000 feature combinations for the GPU as for deep trees (&lt;code&gt;max_depth&lt;/code&gt; = 10) I was getting out of memory errors.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grid set-up&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmark_input &amp;lt;- expand.grid(
  cores = c(4, 16, 32),
  rowsample = c(1e3, 1e4, 2.5e4, 5e4, 7.5e4, 1e5),
  colsample = c(10, 100, 1000),
  trees = c(10, 100, 1000, 10000),
  gpu = c(FALSE),
  rep = 1
) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  {bind_rows(., 
             filter(., cores == 4, colsample &amp;lt; 1000) %&amp;gt;% 
             mutate(gpu = TRUE))} %&amp;gt;% 
  mutate(size = rowsample * colsample * trees) %&amp;gt;% 
  arrange(desc(size), cores)

benchmark_input&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 264 x 7
##    cores rowsample colsample trees gpu     rep          size
##    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt; &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
##  1     4    100000      1000 10000 FALSE     1 1000000000000
##  2    16    100000      1000 10000 FALSE     1 1000000000000
##  3    32    100000      1000 10000 FALSE     1 1000000000000
##  4     4     75000      1000 10000 FALSE     1  750000000000
##  5    16     75000      1000 10000 FALSE     1  750000000000
##  6    32     75000      1000 10000 FALSE     1  750000000000
##  7     4     50000      1000 10000 FALSE     1  500000000000
##  8    16     50000      1000 10000 FALSE     1  500000000000
##  9    32     50000      1000 10000 FALSE     1  500000000000
## 10     4     25000      1000 10000 FALSE     1  250000000000
## # … with 254 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Run benchmark - making use of &lt;code&gt;tibble&lt;/code&gt; nesting and, the always slightly-hacky-feeling, &lt;code&gt;dplyr::rowwise&lt;/code&gt;. Everything here is crudely cached to avoid accidentlly overwriting results.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Cached manually to avoid rerunning on knit.
if (!file.exists(&amp;quot;../../static/data/workstation-benchmark/xgboost.rds&amp;quot;)) {
  benchmark_output_gpu &amp;lt;- benchmark_input %&amp;gt;% 
  rowwise() %&amp;gt;% 
  mutate(bench = list(as_tibble(benchmark_grid(complex_credit_data,
                                       &amp;quot;Status&amp;quot;,
                                        grid = spec_grid,
                                        cores = cores, 
                                        gpu = gpu,
                                        rowsample = rowsample,
                                        trees = trees,
                                        colsample = colsample)))) %&amp;gt;% 
  unnest(bench) %&amp;gt;%
  select(-msg) %&amp;gt;% 
  mutate(duration = toc - tic) %&amp;gt;% 
  filter(fit)
  
  saveRDS(benchmark_output, &amp;quot;../../static/data/workstation-benchmark/xgboost.rds&amp;quot;)
}else{
  benchmark_output &amp;lt;- readRDS( &amp;quot;../../static/data/workstation-benchmark/xgboost.rds&amp;quot;)
}

benchmark_output &amp;lt;- benchmark_output %&amp;gt;% 
  mutate(duration = duration / 60) %&amp;gt;% 
  arrange(gpu, cores) %&amp;gt;% 
  mutate(Compute = paste0(cores, &amp;quot; Threadripper 1950X CPU cores&amp;quot;) %&amp;gt;% 
           ifelse(gpu, &amp;quot;Nvidia 2070 GPU&amp;quot;, .))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;benchmarking-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Benchmarking Results&lt;/h2&gt;
&lt;p&gt;After leaving everything running for a few days, the results are in. The obvious plot to begin with is to split out everything by the number of trees and features and then plot duration against sample numbers for each compute amount (i.e cores and GPU).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmark_output %&amp;gt;% 
  mutate(Cores = factor(cores),
         GPU = gpu) %&amp;gt;% 
  ggplot(aes(rowsample, duration, col = Cores, shape = GPU, group = interaction(Cores, GPU))) +
  geom_point(size = 1.2) +
  geom_line(alpha = 0.8) + 
  facet_grid(colsample ~ trees, scales = &amp;quot;free_y&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;top&amp;quot;, axis.text.x = element_text(angle = 90,hjust = 1)) +
  scale_x_continuous(labels = scales::comma) + 
  scale_color_viridis_d(end = 0.9, begin = 0.1) +
  labs(x = &amp;quot;Rows&amp;quot;,
       y = &amp;quot;Duration (minutes)&amp;quot;,
       caption = &amp;quot;Number of Trees ~ Number of Features&amp;quot;, 
       title = &amp;quot;Xgboost via h2o: Duration&amp;quot;,
       subtitle = &amp;quot;10 model hyper-parameter grid search with 5 fold cross-validation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-26-benchmarking-workstation-xgboost_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;3200&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the first major takeaway is that using the GPU appears to be slower, and mostly much slower, than using 4 CPU cores. This is very surprising to me as everything I have seen elsewhere would indicate that the GPU should offer some substantial speed up. There are some indications however that for larger data sets, and for larger tree numbers, the GPU may be comparable to multiple CPU cores. Potentially this is because any computational benefit from using the GPU is being swamped by the overhead of constantly passing data. Therefore, as the complexity of the problem increases so does the potential benefits of using the GPU. We see something similar for increasing the CPU count, with grids with 10 features running in nearly the same time for 4, 16 and 32 cores, whilst grids with 1000 features are drastically slower on 4 CPUs vs 16. Across all tests it looks like there is little benefit from using 32 (with 16 virtual) over 16 cores.&lt;/p&gt;
&lt;p&gt;To get a closer look at the CPU results and to try and understand the magnitude of the results, I’ve plotted the percentage improvement from a given compute amount over the longest duration for that number of rows - filtering out the GPU results as these would otherwise mask any other findings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmark_output %&amp;gt;% 
  filter(!gpu) %&amp;gt;% 
  group_by(rowsample, colsample, trees) %&amp;gt;% 
  mutate(duration = (max(duration) - duration) / max(duration)) %&amp;gt;% 
  mutate(Cores = factor(cores)) %&amp;gt;%
  ggplot(aes(rowsample, duration, duration, col = Cores)) +
  geom_point(size = 1.2) +
  geom_line(alpha = 0.8) + 
  facet_grid(colsample ~ trees, scales = &amp;quot;free_y&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;top&amp;quot;, axis.text.x = element_text(angle=90,hjust=1)) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(labels = scales::comma) + 
  scale_color_viridis_d(begin = 0.2, end = 0.8) +
  labs(x = &amp;quot;Rows&amp;quot;,
       y = &amp;quot;Performance improvement over baseline (%)&amp;quot;,
       caption = &amp;quot;Number of Trees ~ Number of Features&amp;quot;,
              title = &amp;quot;Xgboost via h2o: Performance over baseline&amp;quot;,
       subtitle = &amp;quot;10 model hyper-parameter grid search with 5 fold cross-validation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-26-benchmarking-workstation-xgboost_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;3200&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For 10 features, the results are very noisy with 4 cores being comparable until 10,000 trees are used. For both 100 and 1000 features, 16 (+32) cores are superior across the board with a near linear speed up as the number of samples increases. Whilst we might imagine that increasing core count from 4 to 16 should result in a near 4 times speed up, interestingly, we are only seeing a maximum speedup of around 70%. This is probably because &lt;code&gt;h2o&lt;/code&gt; is parallelised on the model level (- this is conjecture based on observing &lt;code&gt;htop&lt;/code&gt;), which means that for each fold of each model all the data has to be transferred between cores leading to a large overhead. In these particular test cases, the overhead of passing data and setting up jobs is taking up much of the potential benefit from additional cores. It’s likely that in larger data sets, with longer compute times, this would be less of an issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrap up&lt;/h2&gt;
&lt;p&gt;In this post, I have looked at the performance of &lt;code&gt;xgboost&lt;/code&gt; via &lt;code&gt;h2o&lt;/code&gt; on a sample data set, using a real-world test case of a cross-validated grid search. I found that using the GPU resulted in slower run times across the board, although there was some indication that performance improved for larger data and more complex models. Increasing the physical CPU count to 16 increased performance up to a maximum of 70% over 4 cores (for 100,000 features, 1000 features and 10,000 trees) but adding virtual cores led to no benefit.&lt;/p&gt;
&lt;p&gt;A major takeaway for me is that I probably shouldn’t be relying on &lt;code&gt;h2o&lt;/code&gt; for my grid searching in future. Something to experiment with would be parallelising across the grid, with each model using a single core. Having very much swallowed the Kool-Aid when it comes GPU compute, I was also surprised by how poor the performance was here. This is something to test further as using &lt;code&gt;xgboost&lt;/code&gt; within &lt;code&gt;h2o&lt;/code&gt; makes it difficult to pick apart where the problem lies.&lt;/p&gt;
&lt;p&gt;Any thoughts on these results would be appreciated, especially regarding the poor performance of the GPU. I am also in the market for a new ML toolbox. I’ve been looking at &lt;a href=&#34;https://github.com/mlr-org/mlr&#34;&gt;&lt;code&gt;mlr&lt;/code&gt;&lt;/a&gt; so any recommendations would be appreciated. I’ll be following this post up with another benchmarking post using &lt;code&gt;benchmarkme&lt;/code&gt; in the next few days - if I can resist turning off virtual cores and getting going with some more overclocking.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>getTBinR 0.5.7 now on CRAN - Tuberculosis reports and summary plots</title>
      <link>http://www.samabbott.co.uk/post/gettbinr-5-7/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/gettbinr-5-7/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR 0.5.7&lt;/code&gt;&lt;/a&gt; is now on CRAN and should be available on a mirror near you shortly! This update mainly focussed on building out new country level Tuberculosis (TB) report functionality but along the way this led to a new summary plotting function that quickly and easily shows TB trends across regions and globally. I also had some fun developing a hexsticker (Tweet at me with something you made using the package to get a physical version - whilst my postage money lasts…), reducing the dependencies with &lt;a href=&#34;https://github.com/jimhester/itdepends&#34;&gt;&lt;code&gt;itdepends&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://uptakeopensource.github.io/pkgnet/index.html&#34;&gt;&lt;code&gt;pkgnet&lt;/code&gt;&lt;/a&gt; and dealing with some breaking changes from an uncoming &lt;code&gt;dplyr&lt;/code&gt; update (my own fault for missing a function import).&lt;/p&gt;
&lt;p&gt;The full changelog is below along with an example of the country level TB report generated for the UK (generate a report on the country of your choice using &lt;code&gt;getTBinR::render_country_report(country = &amp;quot;United Kingdom&amp;quot;, save_dir = &amp;quot;.&amp;quot;)&lt;/code&gt;).&lt;/p&gt;
&lt;div id=&#34;feature-updates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature updates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Added support for &lt;code&gt;annual_change&lt;/code&gt; to &lt;code&gt;summarise_tb_burden&lt;/code&gt; and added validating tests.&lt;/li&gt;
&lt;li&gt;Added support for rates and proportions to &lt;code&gt;summarise_tb_burden&lt;/code&gt; and added validating tests.&lt;/li&gt;
&lt;li&gt;Added a new function - &lt;code&gt;plot_tb_burden_summary&lt;/code&gt;. Function wraps &lt;code&gt;summarise_tb_burden&lt;/code&gt; and allows all in one summary plotting. Inspired by this case study.&lt;/li&gt;
&lt;li&gt;Added a rmarkdown parameterised country level report on TB (&lt;code&gt;render_country_report&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Added a report generating button to the dashboard generated by &lt;code&gt;run_tb_dashboard&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Tweaked &lt;code&gt;map_tb_burden&lt;/code&gt; to not use &lt;code&gt;geom_path&lt;/code&gt; for country outlines.&lt;/li&gt;
&lt;li&gt;Added a smooth argument to &lt;code&gt;plot_tb_burden&lt;/code&gt; to allow smooth trend lines to be plotted (derived using &lt;code&gt;ggplot2::geom_smooth&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Tweaked line thickness in &lt;code&gt;plot_tb_burden&lt;/code&gt; to improve plot appearance.&lt;/li&gt;
&lt;li&gt;Added legend argument to all plotting functions to allow control of the legend appearance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;package-updates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Package updates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Added script to generate hexsticker&lt;/li&gt;
&lt;li&gt;Added hexsticker to README&lt;/li&gt;
&lt;li&gt;Added DOI link to Zenodo.&lt;/li&gt;
&lt;li&gt;Updated tests to account for &lt;code&gt;dplyr&lt;/code&gt; 8.0 release and &lt;code&gt;vdiffr&lt;/code&gt; updates.&lt;/li&gt;
&lt;li&gt;Added itdepends to package report functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-united-kingdom-tuberculosis-report&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: United Kingdom Tuberculosis Report&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Load the package
library(getTBinR)
## Load additional packages
library(dplyr) # For data munging 
library(tidyr)
library(rlang)
library(ggplot2)

## Get the data
tb &amp;lt;- get_tb_burden(verbose = FALSE)
## Get the data dictionary
dict &amp;lt;- get_data_dict(verbose = FALSE)
##Assign parameters - these are set in the YAML within the package
country &amp;lt;- &amp;quot;United Kingdom&amp;quot;
interactive &amp;lt;- FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tb-incidence-rates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TB incidence rates&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metric_summary &amp;lt;- function(df = NULL, target_country = NULL, metric = NULL) {
  
  target_country &amp;lt;- df$country[grepl(target_country, df$country)] %&amp;gt;% 
    unique %&amp;gt;% 
    first
    
  ##  Set up metric with confidence intervals
  metric &amp;lt;- enquo(metric)
  metric_lo &amp;lt;- sym(paste0(quo_name(metric), &amp;quot;_lo&amp;quot;))
  metric_hi &amp;lt;- sym(paste0(quo_name(metric), &amp;quot;_hi&amp;quot;))
  
  ## Filter for the country of interest
  country_df &amp;lt;- df %&amp;gt;% 
  filter(country %in% target_country)
  
## Most up to date year of incidence data
recent_inc &amp;lt;- country_df %&amp;gt;% 
  drop_na(!!metric) %&amp;gt;% 
  filter(year == max(year)) %&amp;gt;% 
  select(!!metric, !!metric_lo, !!metric_hi, year, g_whoregion) %&amp;gt;% 
  mutate(inc_rate = paste0(!!metric, &amp;quot; (&amp;quot;, !!metric_lo, &amp;quot; - &amp;quot;, !!metric_hi, &amp;quot;)&amp;quot;))
## Country rank
ranked_countries_inc &amp;lt;- df %&amp;gt;% 
  filter(year == recent_inc$year) %&amp;gt;% 
  arrange(desc(!!metric)) %&amp;gt;% 
  mutate(rank = 1:n())
## World rank
target_rank_world &amp;lt;- ranked_countries_inc %&amp;gt;% 
  filter(country == target_country) %&amp;gt;% 
  pull(rank)
## Region rank
target_rank_region &amp;lt;- ranked_countries_inc %&amp;gt;% 
  filter(g_whoregion %in% recent_inc$g_whoregion) %&amp;gt;% 
  mutate(rank = 1:n()) %&amp;gt;% 
  filter(country == target_country) %&amp;gt;%
  pull(rank)
## Summarise annual change
country_change &amp;lt;- summarise_tb_burden(metric = quo_name(metric),
                                      stat = &amp;quot;mean&amp;quot;,
                                      countries = target_country,
                                      compare_to_region = FALSE,
                                      compare_to_world = FALSE,
                                      compare_all_regions = FALSE,
                                      annual_change = TRUE,
                                      verbose = FALSE) %&amp;gt;% 
  filter(year &amp;gt; (max(year) - 10)) %&amp;gt;% 
  summarise(change = mean(!!metric, na.rm = FALSE)) %&amp;gt;%
  mutate(change = round(change * 100, 1) %&amp;gt;% 
           paste0(., &amp;quot;%&amp;quot;)) %&amp;gt;% 
  pull(change)
out &amp;lt;- list(recent_inc$year[1], recent_inc$inc_rate[1],
            target_rank_world, target_rank_region,
            country_change)
names(out) &amp;lt;- c(&amp;quot;year&amp;quot;, &amp;quot;metric&amp;quot;, &amp;quot;world_rank&amp;quot;, &amp;quot;region_rank&amp;quot;, &amp;quot;avg_change&amp;quot;)
out &amp;lt;- ifelse(is.na(out), &amp;quot;(Missing)&amp;quot;, out)
return(out)
}
  
  
inc_sum &amp;lt;- metric_summary(tb, country, e_inc_100k)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In 2017 United Kingdom had an estimated Tuberculosis incidence rate of 8.9 (8.1 - 9.8) per 100,000 people making it number 165 in the world and number 32 regionally. In the last 10 years this has changed by -4.9% on average each year.&lt;/p&gt;
&lt;div id=&#34;regional-and-global-trends-comparision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regional and Global Trends Comparision&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_summary(countries = country,
                       metric_label = &amp;quot;e_inc_100k&amp;quot;,
                       compare_to_world = TRUE, 
                       compare_to_region = TRUE,
                       compare_all_regions = FALSE,
                       annual_change = FALSE,
                       facet = &amp;quot;Area&amp;quot;,
                       scales = &amp;quot;free_y&amp;quot;,
                       legend = &amp;quot;none&amp;quot;,
                       interactive = interactive,
                       verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;rates-regional-breakdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rates Regional Breakdown&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_overview(countries = country,
                        compare_to_region = TRUE,
                        interactive = interactive,
                        verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-detection-rates-cdr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case Detection Rates (CDR)&lt;/h2&gt;
&lt;p&gt;United Kingdom had an estimated case detection rate of 89 (81 - 98)% in 2017 making it number 4 in the world (with number 1 having the highest CDR) and number 3 regionally. In the last 10 years this has changed by 0% on average each year.&lt;/p&gt;
&lt;div id=&#34;regional-breakdown&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regional Breakdown&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_overview(metric = &amp;quot;c_cdr&amp;quot;,
                        countries = country,
                        compare_to_region = TRUE,
                        interactrive = interactive,
                        verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tb-mortality-rates---excluding-hiv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TB mortality rates - excluding HIV&lt;/h2&gt;
&lt;p&gt;In 2017 United Kingdom had an estimated Tuberculosis mortality rate (excluding HIV) of 0.53 (0.52 - 0.53) per 100,000 people making it number 166 in the world and number 32 regionally. In the last 10 years this has changed by -1.6% on average each year.&lt;/p&gt;
&lt;div id=&#34;proportion-of-tb-cases-that-died-excluding-hiv---regional-and-global-comparision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Proportion of TB Cases that Died (excluding HIV) - Regional and Global Comparision&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_summary(metric = &amp;quot;e_mort_exc_tbhiv_num&amp;quot;,
                       denom = &amp;quot;e_inc_num&amp;quot;,
                       rate_scale = 100,
                       countries = country,
                       compare_to_region = TRUE,
                       compare_all_regions = FALSE,
                       interactive = interactive,
                       verbose = FALSE,
                       facet = &amp;quot;Area&amp;quot;,
                       scales = &amp;quot;free_y&amp;quot;,
                       legend = &amp;quot;none&amp;quot;) +
  labs(y = &amp;quot;Proportion (%) of TB cases that died (excluding HIV)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;3840&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rates-regional-breakdown-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rates Regional Breakdown&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_overview(metric = &amp;quot;e_mort_exc_tbhiv_100k&amp;quot;,
                        countries = country,
                        compare_to_region = TRUE,
                        interactrive = interactive,
                        verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tb-hiv-related-mortality-rates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TB HIV related mortality rates&lt;/h2&gt;
&lt;p&gt;In 2017 United Kingdom had an estimated Tuberculosis mortality rate (related to HIV) of 0.1 (0.05 - 0.16) per 100,000 people making it number 127 in the world and number 23 regionally. In the last 10 years this has changed by 7.6% on average each year.&lt;/p&gt;
&lt;div id=&#34;proportion-of-tb-cases-that-died-related-to-hiv---regional-and-global-comparision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Proportion of TB Cases that Died (related to HIV) - Regional and Global Comparision&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_summary(metric = &amp;quot;e_mort_tbhiv_num&amp;quot;,
                       denom = &amp;quot;e_inc_num&amp;quot;,
                       rate_scale = 100,
                       countries = country,
                       compare_to_region = TRUE,
                       compare_all_regions = FALSE,
                       interactive = interactive,
                       verbose = FALSE,
                       facet = &amp;quot;Area&amp;quot;,
                       scales = &amp;quot;free_y&amp;quot;,
                       legend = &amp;quot;none&amp;quot;) +
  labs(y = &amp;quot;Proportion (%) of TB cases that died (related to HIV)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;3840&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rates-regional-breakdown-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rates Regional Breakdown&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_overview(metric = &amp;quot;e_mort_tbhiv_100k&amp;quot;,
                        countries = country,
                        compare_to_region = TRUE,
                        interactrive = interactive,
                        verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-22-gettbinr-5-7_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;2560&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For other examples of using &lt;code&gt;getTBinR&lt;/code&gt; to visualise the WHO TB data see &lt;a href=&#34;https://gist.github.com/seabbs&#34;&gt;my&lt;/a&gt; gists, previous blog &lt;a href=&#34;https://www.samabbott.co.uk/tags/who/&#34;&gt;posts&lt;/a&gt;, and the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building an Rstats Workstation</title>
      <link>http://www.samabbott.co.uk/post/building-an-rstats-workstation/</link>
      <pubDate>Sun, 23 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/building-an-rstats-workstation/</guid>
      <description>


&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;I regularly use cloud resources (AWS and GCP) both in my day job and for personal projects but recently I have been finding that having to spin up a cloud instance for quick analysis can be tedious, even when making use of tools for reproducibility like docker. This is particularly the case for self-learning when spending money on cloud resources feels wasteful, especially when I have half an eye on something else (i.e the TV). Often I find that this leads me to not look at the things that I am interested in, although this could also just be my own lack of motivation!&lt;/p&gt;
&lt;p&gt;It seemed sensible to finally bite the bullet and build a local workstation that could handle all the compute tasks I can throw at it (up to a point obviously). At the moment I am looking to further explore &lt;a href=&#34;http://www.stats.ox.ac.uk/%7Edoucet/andrieu_doucet_holenstein_PMCMC.pdf&#34;&gt;P-MCMC methods&lt;/a&gt;, Deep learning and get involved in a few &lt;a href=&#34;https://www.kaggle.com&#34;&gt;kaggle&lt;/a&gt; competitions. These needs inform my parts choices below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;requirements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A high core number for compute intensive CPU tasks that can easily be parallel enabled such as Sequential Monte Carlo and distributed machine learning (&lt;code&gt;xgboost&lt;/code&gt;, &lt;code&gt;h2o&lt;/code&gt; etc).&lt;/li&gt;
&lt;li&gt;Good single core CPU performance as many tasks are natively single core (i.e R).&lt;/li&gt;
&lt;li&gt;Enough RAM to support the CPU cores but this is an area that I could economise as more RAM can be added at a later date. Typically 2Gbs per core is the minimum.&lt;/li&gt;
&lt;li&gt;A GPU with good support and sufficient power to do meaningful deep learning. As of my current reading this essentially means a Nvidia GPU.&lt;/li&gt;
&lt;li&gt;A fast hard drive but a large one is not needed as only the data currently used in analysis needs to be stored (on top of the OS and applications).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;parts-list&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parts list&lt;/h2&gt;
&lt;p&gt;Knowing nothing about building PCs, or PC components, I started using a suggested build on &lt;a href=&#34;https://pcpartpicker.com/guide/NKV323/gaming-streaming-and-editing-build&#34;&gt;pcpartpicker&lt;/a&gt; for streaming and editing. This was a good match as editing/streaming require a larger core number than other use cases, whilst still needing a strong GPU. From here, I went through each part in turn and reviewed the alternatives. The main resources that I used for PC part reviews and thoughts were: the &lt;a href=&#34;https://thewirecutter.com&#34;&gt;thewirecutter&lt;/a&gt;, &lt;a href=&#34;https://www.anandtech.com&#34;&gt;anandtech&lt;/a&gt; and &lt;a href=&#34;https://www.tomshardware.com&#34;&gt;tomshardware&lt;/a&gt;. From this reading, I settled on a Threadripper CPU from the previous generation as this provided 16 cores, relatively good single core performance and some scope for further overclocking. Another viable option was the current generation Threadripper, which has improved speed and better automatic overclocking but ultimately I decided that the cost/benefit didn’t make sense. The choice of CPU dictated the motherboard choice, which I selected based on the parametric filters pcpartpicker provides (a real lifesaver). As the Threadrippers are very heat intensive I went with a water cooler for the CPU (&lt;em&gt;Note: I probably should have chosen a bigger radiator here for more cooling&lt;/em&gt;), again selected based on the parametric filters + reviews. I went with 32GB of 3200 Mhz RAM as the minimum required for this many CPU cores, with a good balance of speed and price. I chose the Nvidia RTX 2070 Windforce for GPU after looking at some benchmarks and because the 2080 was dramatically more expensive. The Windforce also comes with a slightly higher clock speed than other entry level 2070s along with 3 fans for additional cooling. Finally, I chose the best reviewed M2 SSD that I could find as the read/write speeds are dramatically faster than traditional SSDs, going with a smaller disk size (500 Gb) to minimise the cost. The final parts list is &lt;a href=&#34;https://uk.pcpartpicker.com/list/9FpFMZ&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/2018-12-21-building-an-rstats-workstation/parts.png&#34; alt=&#34;All the parts (excluding the case) ready to go.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;All the parts (excluding the case) ready to go.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I was slightly nervous about the build process but after reading the manuals for all of the parts (quite dull - the motherboard and case manuals turned out to be the most helpful) and watching multiple youtube videos going through the PC building process (there are lots of great channels providing great resources) it went without a hitch. In brief the build process boiled down to the following.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Installing the CPU into the motherboard&lt;/li&gt;
&lt;li&gt;Installing the RAM into the motherboard&lt;/li&gt;
&lt;li&gt;Installing the SSD into the motherboard (as using an M.2 SSD).&lt;/li&gt;
&lt;li&gt;Installing the motherboard into the case&lt;/li&gt;
&lt;li&gt;Adding the CPU cooler to the case&lt;/li&gt;
&lt;li&gt;Plugging the fans into the correct motherboard sockets&lt;/li&gt;
&lt;li&gt;Plugging the case ports into the motherboard&lt;/li&gt;
&lt;li&gt;Installing the GPU into the motherboard&lt;/li&gt;
&lt;li&gt;Installing the power supply and linking up the power connections&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This all sounds very simple and, amazingly, it really was. After going through this process over a few hours (most of the time was spent looking for screws and getting confused over which instruction manual to use), all the parts were installed and everything powered up correctly (having RGB was very reassuring here).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;os&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;OS&lt;/h2&gt;
&lt;p&gt;Historically, I have been a Mac user but recently I have been using cloud resources and docker more and more. Both of these use Linux so this seemed like a sensible choice. Not knowing much about the various distros available I wanted to go with one based on Debian, as these use the same package libraries etc. that I have been using on AWS and elsewhere. I settled on &lt;a href=&#34;https://ubuntubudgie.org/downloads&#34;&gt;Ubuntu Budgie&lt;/a&gt;, which is a theme for standard Ubuntu (one of the most commonly used distros). Budgie provides a modern, lightweight interface, that overlays the core Ubuntu distro. I followed &lt;a href=&#34;https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-ubuntu&#34;&gt;a guide&lt;/a&gt; for creating an OS USB boot and then followed the installation instructions after rebooting the workstation (&lt;em&gt;Note: During installation I chose to encrypt the boot disk. This means that the computer requires a password before booting - meaning that it can’t be remotely rebooted. I would definitely not choose to do this in the future.&lt;/em&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;software&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Software&lt;/h2&gt;
&lt;p&gt;After completing the Ubuntu set up and installing the usual suspects (i.e Dropbox etc.), my first requirement was something to reproduce the functionality of &lt;a href=&#34;https://www.alfredapp.com&#34;&gt;alfred&lt;/a&gt; (an amazing Mac only spotlight replacement that I completely rely on for navigating my computer). After a brief search, I came across &lt;a href=&#34;https://albertlauncher.github.io/docs/installing/&#34;&gt;albert&lt;/a&gt; a great open source Linux alternative. The next step was to get my most commonly used Linux command line tools, namely &lt;code&gt;htop&lt;/code&gt; (to monitor CPU usage) and &lt;code&gt;tree&lt;/code&gt; (to explore the file system). These were installed in the terminal with the following:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install htop tree&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check that everything was working correctly in the build, I needed to be able to monitor CPU temperatures, both ideal and under load. I chose to do this using &lt;a href=&#34;https://github.com/amanusk/s-tui&#34;&gt;&lt;code&gt;s-tui&lt;/code&gt;&lt;/a&gt; - a great tool that monitors CPU usage, temperatures, power consumption and speed. It provides an interface to the &lt;code&gt;stress&lt;/code&gt; package, allowing load scenarios to be simulated. I also needed to be able to check that the GPU was working as expected. To do this, I used the following command in the terminal: &lt;code&gt;watch -n 5 nvidia-smi&lt;/code&gt; - this calls &lt;code&gt;nvidia-smi&lt;/code&gt; every 5 seconds. &lt;code&gt;nvidia-smi&lt;/code&gt; comes prepackaged with the Nvidia drivers (you may need to update/switch your drivers - I used &lt;a href=&#34;http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux&#34;&gt;this post&lt;/a&gt;) and allows monitoring of Nvidia GPUs (much like &lt;code&gt;top&lt;/code&gt; for CPUs).&lt;/p&gt;
&lt;p&gt;As this workstation is mostly going to be running analyses via &lt;a href=&#34;https://hub.docker.com/r/rocker/tidyverse&#34;&gt;rocker&lt;/a&gt;-based docker containers, the next key step is to download and install &lt;a href=&#34;https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce-1&#34;&gt;Docker CE&lt;/a&gt;. See the Docker documentation for more details on using docker (well worth the time as Docker is the one of the best tools for ensuring that analysis is reproducible). If everything is installed and working correctly, the following should lead to an instance of an Rstudio server at &lt;code&gt;localhost:8787&lt;/code&gt; with the username and password of &lt;code&gt;seabbs&lt;/code&gt; (you may need to use &lt;code&gt;sudo&lt;/code&gt; here).&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker run -d -p 8787:8787 -e USER=seabbs -e PASSWORD=seabbs --name rstudio rocker/tidyverse&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here, you should have everything you need to do Rstats using your new workstation! The final software that is required is &lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;&lt;code&gt;nvidia-docker&lt;/code&gt;&lt;/a&gt;, this provides a docker wrapper that allows docker containers to access the Nvidia GPU. After installing, check its working using the following:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;overclocking-and-stress-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overclocking and stress tests&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./img/2018-12-21-building-an-rstats-workstation/benchmarks.png&#34; alt=&#34;Stress testing the CPU and GPU using s-tui and ethminer&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Stress testing the CPU and GPU using &lt;code&gt;s-tui&lt;/code&gt; and &lt;code&gt;ethminer&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As the 1950X Threadripper has a relatively low clock speed of 3.4 Mhz, there is general agreement online that with proper cooling there is some good overhead for overclocking. Depending on the use case, this may or may not be worth doing as for mostly low core work flows the turbo-boosting (not available when overclocked) will give improved performance over most overclocks. In my case, I am most interested in optimizing multicore use and am willing to sacrifice a small boost in single core performance (of around 5%) to achieve this.&lt;/p&gt;
&lt;p&gt;In order to check that an overclock is both stable and does not increase the temperature of the system to dangerous levels, it is important to stress test both the CPU and GPU over an extended time period (I initially started with an hour and then extended this to 6 hours for the final test). To do this I used &lt;code&gt;s-tui&lt;/code&gt; to both monitor and stress the CPU, &lt;a href=&#34;https://github.com/ethereum-mining/ethminer&#34;&gt;&lt;code&gt;ethminer&lt;/code&gt;&lt;/a&gt; to stress the GPU by mining Ethereum and &lt;code&gt;nvidia-smi&lt;/code&gt; to monitor the GPU temperature. To overclock the system, I made &lt;a href=&#34;https://overclocking.guide/gigabyte-threadripper-overclocking-guide/&#34;&gt;adjustments in the BIOS&lt;/a&gt;, starting with the RAM (to get it running at its maximum specifications, which is not the default), and then moving to increasing the CPU speed by 100 Mhz intervals each time. Following this process, I got to a speed of 3.8 Mhz (see the picture above for the stress test at this level). Going higher than this required an increase in voltage for stability and this lead to dramatic temperature increases (beyond 70C). A possible option is turning off simultaneous multithreading (i.e going from 32 virtual cores to 16 real cores), this would reduce power consumption and allow for a higher overclock at the cost of reduced potential parallelisation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;static-ip-and-remote-access&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Static IP and Remote Access&lt;/h1&gt;
&lt;p&gt;This workstation will primarily be used headlessly from my Macbook Pro so the next important step is to set up remote access. I generally like to connect over SSH but sometimes it is very useful to have an GUI interface to work with. To allow this I installed a VNC server (&lt;a href=&#34;https://www.realvnc.com/en/&#34;&gt;RealVNC VNC Server&lt;/a&gt;). The nice thing about this is that it works out of the box both on the local network and externally. For the SSH setup, I hardened my configuration using &lt;a href=&#34;https://help.ubuntu.com/community/SSH/OpenSSH/Configuring&#34;&gt;this&lt;/a&gt; post (I also changed my SSH port from 22 to something else to limit automatic detection). After following these steps, the workstation now needs the public keys of any computers that you want to connect to it using SSH - some tips for this can be found &lt;a href=&#34;https://help.ubuntu.com/community/SSH/OpenSSH/Keys&#34;&gt;here&lt;/a&gt;. To make connections to the workstation consistent it needs a static IP. I did this using the &lt;a href=&#34;https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux&#34;&gt;Ubuntu interface&lt;/a&gt; rather than using my router as I usually would and it was so easy that I will definitely favour this option in the future. The next step is to allow SSH connection from your computer when your not at home. To do this you need to open a port in your router and link it to the SSH port of the workstation (see the instructions for your router). If, like me, you don’t have a static IP address then you need to find a workaround. I used &lt;a href=&#34;https://www.noip.com&#34;&gt;noip&lt;/a&gt; to provide an address to SSH to rather than an IP. Finally, I like to add aliases to avoid having to write out the IP each time I want to SSH. Below I have an example setup (this needs to be copied into the &lt;code&gt;.profile&lt;/code&gt; file).&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;alias archie_internal=&amp;#39;ssh -p 2345 user@your-local-ip&amp;#39;
alias archie=&amp;#39;ssh -p 2345 user@computer.hopto.org&amp;#39;
alias archie_with_ports=&amp;#39;ssh -p 2345 -l localhost:8787:localhost:8787 user@computer.hopto.org&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Test your connection from the connecting computer using the following:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;archie_with_ports&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should be able to control the workstation in the terminal and see Rstudio at &lt;code&gt;localhost:8787&lt;/code&gt; (This post was written remotely using this approach and the &lt;code&gt;seabbs/seabbs.github.io&lt;/code&gt; docker container).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Next steps&lt;/h1&gt;
&lt;p&gt;Whilst I now have a working workstation there are still some things that need sorting out. The most important of these are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work out how to use GPUs in arbitrary docker containers without first installing cuda etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Suggestions for how best to use &lt;a href=&#34;https://twitter.com/nvidia?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@nvidia&lt;/span&gt;&lt;/a&gt; GPUs in rocker &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/docker?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#docker&lt;/a&gt; containers. Ideally looking for a solution using nvidia docker compose rather than manually installing cuda into each container? Does this exist or are there good alternatives?
&lt;/p&gt;
— Sam Abbott (&lt;span class=&#34;citation&#34;&gt;@seabbs&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/seabbs/status/1075709270979686401?ref_src=twsrc%5Etfw&#34;&gt;December 20, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Benchmark GPU vs CPU for common tasks such as training &lt;a href=&#34;https://xgboost.ai&#34;&gt;Xgboost models&lt;/a&gt; and using &lt;a href=&#34;https://xgboost.ai&#34;&gt;libBi&lt;/a&gt; (for P-MCMC and SMC-SMC).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Explore whether simultaneous multi-threading (hyper-threading) leads to performance improvements in common Rstats work loads. For Intel CPUs, my experience is that only using real cores leads to better performance for intensive compute tasks. If this is the case here then this will allow for greater overclocking headroom.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;things-to-improve-next-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Things to improve next time&lt;/h1&gt;
&lt;p&gt;This was a huge learning experience (which I really enjoyed). My main takeaways so far have been:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Think hard about parts trade-off&lt;/strong&gt; - for example, I went with the last generation Threadripper as a cost saving initiative. In retrospect, the latest generation might have been worth the money because the new automatic overclocking features mean that significant improves are likely for most workloads.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Look carefully at cooling options&lt;/strong&gt; - for example, a larger radiator would give more head room from CPU overclocking.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;wrap-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrap Up&lt;/h1&gt;
&lt;p&gt;If you got through this post then thanks for reading! Hopefully it gave some insights into how to approach building out a workstation for Rstats. I am definitely not an expert so any thoughts would be very welcome. As mentioned above, I am particularly interested in performance comparisons between GPUs and CPUs (both real and virtual cores) and so would welcome any insights into this aspect of things.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>getTBinR 0.5.5 now on CRAN - 2017 data.</title>
      <link>http://www.samabbott.co.uk/post/gettbinr-5-5/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/gettbinr-5-5/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;getTBinR 0.5.5&lt;/code&gt;&lt;/a&gt; is now on CRAN and should be available on a mirror near you shortly! This update is mainly about highlighting the availability of TB data for 2017, although some small behind the scenes changes were required to get the code set up going forward for yearly updates. A few more plotting options have been added, along with the corresponding tests (definitely the most exciting news). The full changelog is below along with a short example highlighting some of the changes in the 2017 data.&lt;/p&gt;

&lt;p&gt;The main message from the 2017 data is that in 2017 there were again over 10 million estimated TB cases globally with only a 1.8% decrease in incidence rates compared to 2016. Over the last 10 years progress has been made with an average of a 1.9% decrease in TB incidence rates year on year. However there is little evidence of an increase in the rate that TB incidence rates are falling, with incidence rates forecast to remain at over 100 per 100,000 for the next 10 years if progress is made at the same rate as in the last decade.&lt;/p&gt;

&lt;h2 id=&#34;feature-updates&#34;&gt;Feature updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Added a years filter to &lt;code&gt;plot_tb_burden&lt;/code&gt; and &lt;code&gt;plot_tb_burden_overview&lt;/code&gt;. This allows a range of years to be plotted. The default is all years which was the previous de facto default.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;package-updates&#34;&gt;Package updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Updated docs to reflect new year of data.&lt;/li&gt;
&lt;li&gt;Updated examples to use the new year of data as standard.&lt;/li&gt;
&lt;li&gt;Updated README to always use the current year of data.&lt;/li&gt;
&lt;li&gt;Updated all vignettes to reflect new data or be fixed to historic data as appropriate.&lt;/li&gt;
&lt;li&gt;Update site with links out to blog posts using the newest version of &lt;a href=&#34;http://pkgdown.r-lib.org&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;pkgdown&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;example-changes-in-tb-incidence-rates-in-2017&#34;&gt;Example: Changes in TB incidence rates in 2017&lt;/h2&gt;

&lt;p&gt;The code below quickly explores the updated data by first estimating global incidence rates and the annual change between years. The country level annual changes in TB incidence rates are then plotted, first globally and then by region. Finally, the trend in incidence rates is explored using country, regional and global level TB incidence rates. See &lt;a href=&#34;https://www.samabbott.co.uk/img/getTBinR/storyboard-5-5.png&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for a full size version of the storyboard.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Get required packages - managed using pacman
if (!require(pacman)) install.packages(&amp;quot;pacman&amp;quot;); library(pacman)
p_load(&amp;quot;getTBinR&amp;quot;)
p_load(&amp;quot;ggplot2&amp;quot;)
p_load(&amp;quot;ggrepel&amp;quot;)
p_load(&amp;quot;scales&amp;quot;)
p_load(&amp;quot;viridis&amp;quot;)
p_load(&amp;quot;dplyr&amp;quot;)
p_load(&amp;quot;forcats&amp;quot;)
p_load(&amp;quot;ggridges&amp;quot;)
p_load_gh(&amp;quot;thomasp85/patchwork&amp;quot;)

##Pull TB data 
tb_burden &amp;lt;- get_tb_burden() 


## Summarise global changes
global_tb &amp;lt;- tb %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  summarise_at(.vars = c(&amp;quot;e_inc_num&amp;quot;, &amp;quot;e_pop_num&amp;quot;), ~ sum(., na.rm = TRUE)) %&amp;gt;% 
  mutate(inc_rate = e_inc_num / e_pop_num * 1e5,
         per_change_inc = (inc_rate - lag(inc_rate)) / lag(inc_rate)) %&amp;gt;% 
  mutate(g_whoregion = &amp;quot;Global&amp;quot;,
         label = ifelse(year == max(year), g_whoregion, &amp;quot;&amp;quot;))



global_tb

## TB in 2017
tb_2017 &amp;lt;- global_tb %&amp;gt;% 
  filter(year == 2017)

tb_2017

## Global annual change
global_annual_change &amp;lt;- ggplot(global_tb, aes(year, per_change_inc)) +
  geom_smooth(se = FALSE, col = &amp;quot;black&amp;quot;, size = 1.2, alpha.line = 0.7) +
  geom_point(size = 1.2, alpha = 0.8, col = &amp;quot;black&amp;quot;) +
  scale_y_continuous(label = scales::percent, minor_breaks = NULL, breaks = seq(-0.025, 0, 0.0025)) +
  theme_minimal() +
  labs(
    y = &amp;quot;Annual Percentage Change&amp;quot;,
    x = &amp;quot;Year&amp;quot;,
    title = &amp;quot;Global Annual Percentage Change in Tuberculosis Incidence Rates&amp;quot;,
    caption = &amp;quot;&amp;quot;
    
  )

global_annual_change

## Remove countries with incidence below 1000 or incidence rates below 10 per 100,000 to reduce noise and cal country level annual change.
countries_with_tb_burden &amp;lt;- tb_burden %&amp;gt;% 
  filter(year == 2017,
         e_inc_100k &amp;gt; 10,
         e_inc_num &amp;gt; 1000)

tb_annual_change &amp;lt;- tb_burden %&amp;gt;% 
  semi_join(countries_with_tb_burden, by = &amp;quot;country&amp;quot;) %&amp;gt;% 
  group_by(country) %&amp;gt;% 
  arrange(year) %&amp;gt;% 
  select(year, g_whoregion, country, e_inc_100k, e_inc_num, e_pop_num) %&amp;gt;% 
  mutate(annual_change = (e_inc_100k - lag(e_inc_100k)) / lag(e_inc_100k)) %&amp;gt;% 
  ungroup

## Function to plot annual change
plot_annual_change &amp;lt;- function(df, strat = NULL, subtitle = NULL, years = 2000:2017) {
  dist &amp;lt;- df %&amp;gt;% 
    filter(year %in% years) %&amp;gt;% 
    rename(Region = g_whoregion) %&amp;gt;% 
    mutate(year = year %&amp;gt;% 
             factor(ordered = TRUE) %&amp;gt;% 
             fct_rev) %&amp;gt;% 
    ggplot(aes_string(x = &amp;quot;annual_change&amp;quot;, y = &amp;quot;year&amp;quot;, col = strat, fill = strat)) +
    geom_density_ridges(quantile_lines = TRUE, quantiles = 2, alpha = 0.6) +
    scale_color_viridis(discrete = TRUE, end = 0.9) +
    scale_fill_viridis(discrete = TRUE, end = 0.9) +
    geom_vline(xintercept = 0, linetype = 2, alpha = 0.6) +
    scale_x_continuous(labels = scales::percent, breaks = seq(-0.4, 0.4, 0.1),
                       limits = c(-0.4, 0.4), minor_breaks = NULL) +
    theme_minimal() +
    theme(legend.position = &amp;quot;none&amp;quot;) +
    labs(x = paste0(&amp;quot;Annual Change in &amp;quot;, search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition),
         y = &amp;quot;Year&amp;quot;,
         title = &amp;quot;Annual Percentage Change in Tuberculosis Incidence Rates&amp;quot;,
         subtitle = subtitle,
         caption = &amp;quot;&amp;quot;)
  
  return(dist)
}

## Overall country level annual change
overall &amp;lt;- plot_annual_change(tb_annual_change, NULL,
                              years = seq(2001, 2017, 2), subtitle = &amp;quot;By Country&amp;quot;) 

overall

## Regional country level annual change
region &amp;lt;-  plot_annual_change(tb_annual_change, &amp;quot;Region&amp;quot;,
                              subtitle = &amp;quot;By Region&amp;quot;, 
                              years = seq(2001, 2017, 2)) + 
  facet_wrap(~Region) +
  labs(caption = &amp;quot;&amp;quot;)

region

## Regional and Global TB incidence rates over time
regional_incidence &amp;lt;- tb_burden %&amp;gt;% 
  group_by(g_whoregion, year) %&amp;gt;% 
  summarise(inc = sum(e_inc_num, na.rm = TRUE), pop = sum(e_pop_num, na.rm = TRUE)) %&amp;gt;% 
  ungroup %&amp;gt;% 
  mutate(inc_rate = inc / pop * 1e5) %&amp;gt;% 
  mutate(label = ifelse(year == max(year), g_whoregion, &amp;quot;&amp;quot;)) %&amp;gt;% 
  ggplot(aes(year, inc_rate, col = g_whoregion)) +
  geom_line(alpha = 0.8, size = 1.2) +
  scale_color_viridis(discrete = TRUE, end = 0.9) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  scale_y_continuous(breaks = seq(0, 400, 25), minor_breaks = NULL, limits = c(0, NA)) +
  scale_x_continuous(breaks = seq(2000, 2017, 1), minor_breaks = NULL) +
  geom_text_repel(aes(label = label),
                  nudge_x = 4,
                  force = 10,
                  na.rm = TRUE,
                  min.segment.length = 10) +
  labs(x = search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition,
       y = &amp;quot;Year&amp;quot;,
       title = &amp;quot;Tuberculosis Incidence Rates&amp;quot;,
       subtitle = &amp;quot;By Region, per 100,000 population&amp;quot;) +
  geom_line(data = global_tb, aes(col = NULL), alpha = 0.8, size = 1.2) +
  geom_text_repel(data = global_tb,
                  aes(label = label, col = NULL),
                  nudge_x = 2,
                  nudge_y = 8,
                  na.rm = TRUE)

## Map global TB incidence rates for 2017 using getTBinR
map &amp;lt;- map_tb_burden(year = c(2005, 2009, 2013, 2017), facet = &amp;quot;year&amp;quot;) +
  theme(strip.background = element_blank()) +
  labs(caption = &amp;quot;&amp;quot;,
       title = &amp;quot;Tuberculosis Incidence Rates&amp;quot;,
       subtitle = &amp;quot;By Country, per 100,000 population&amp;quot;)

## Compose storyboard
storyboard &amp;lt;- (map + regional_incidence + plot_layout(widths = c(2, 1))) /
  (region + (global_annual_change /
               overall + labs(caption = &amp;quot;For country level annual percentages change countries with incidence above 1000 and an incidence rate above 10 per 100,000 are shown.
                    The global annual percentage change is shown with a LOESS fit. 
                    By @seabbs | Made with getTBinR | Source: World Health Organisation&amp;quot;)) + plot_layout(widths = c(2, 1))) +
  plot_layout(widths = c(1, 1))

## Save storyboard
ggsave(&amp;quot;storyboard.png&amp;quot;,
       storyboard, width = 20, height = 15, dpi = 330)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./img/getTBinR/storyboard-5-5.png&#34; alt=&#34;getTBinR 0.5.5 storyboard&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For other examples of using &lt;code&gt;getTBinR&lt;/code&gt; to visualise the WHO TB data see &lt;a href=&#34;https://gist.github.com/seabbs&#34; target=&#34;_blank&#34;&gt;my&lt;/a&gt; gists, previous blog &lt;a href=&#34;https://www.samabbott.co.uk/tags/who/&#34; target=&#34;_blank&#34;&gt;posts&lt;/a&gt;, and the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>getTBinR 0.5.4 now on CRAN - new data, map updates and a new summary function.</title>
      <link>http://www.samabbott.co.uk/post/gettbinr-5-4/</link>
      <pubDate>Wed, 16 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/gettbinr-5-4/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;getTBinR 0.5.4&lt;/code&gt;&lt;/a&gt; is now on CRAN and should be available on a mirror near you shortly! This update includes an additional data set for 2016 containing variables related to drug resistant Tuberculosis, some aesthetic updates to mapping functionality and a new &lt;code&gt;summarise_tb_burden&lt;/code&gt; function for summarising TB metrics. Behind the scenes there has been an extensive test overhaul, with &lt;a href=&#34;https://github.com/lionel-/vdiffr&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;vdiffr&lt;/code&gt;&lt;/a&gt; being used to test images, and several bugs fixes. See below for a full list of changes and some example code exploring the new functionality.&lt;/p&gt;

&lt;h2 id=&#34;feature-updates&#34;&gt;Feature updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Added MDR-TB data for 2016, see &lt;a href=&#34;http://www.who.int/tb/country/data/download/en/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for the dataset. The MDR-TB data is automatically joined to the WHO TB burden data.&lt;/li&gt;
&lt;li&gt;Aesthetic updates to &lt;code&gt;map_tb_burden&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Added new &lt;code&gt;summarise_tb_burden&lt;/code&gt; function for summarising metrics across regions, across custom groups and globally.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;package-updates&#34;&gt;Package updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Improved data cleaning, converting &lt;code&gt;Inf&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt; values to &lt;code&gt;NA&lt;/code&gt; when the data is read in.&lt;/li&gt;
&lt;li&gt;Added &lt;code&gt;pgknet&lt;/code&gt; report.&lt;/li&gt;
&lt;li&gt;Improved test robustness and scope&lt;/li&gt;
&lt;li&gt;Added &lt;code&gt;vdiffr&lt;/code&gt; to test plots when not on CRAN.&lt;/li&gt;
&lt;li&gt;Fixed bug for &lt;code&gt;map_tb_burden&lt;/code&gt; which was adding duplicate variables which caused map build to fail.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;example-exploring-global-tb-incidence-rates&#34;&gt;Example: Exploring Global TB Incidence Rates&lt;/h2&gt;

&lt;p&gt;For a quick example the code below pulls the WHO TB data, summarises it by region using &lt;code&gt;getTBinR&lt;/code&gt; and &lt;a href=&#34;https://github.com/clauswilke/ggridges&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;ggridges&lt;/code&gt;&lt;/a&gt;, maps global TB incidence rates in 2016, and finally plots an overview of incidence rates in the 10 countries with highest TB incidence rates in 2016. The figures generated are then combined into a single storyboard using the &lt;a href=&#34;https://github.com/thomasp85/patchwork&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;pathwork&lt;/code&gt;&lt;/a&gt; package. See &lt;a href=&#34;https://www.samabbott.co.uk/img/getTBinR/storyboard-5-4.png&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for a full size version of the storyboard.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
## Get required packages - managed using pacman
if (!require(pacman)) install.packages(&amp;quot;pacman&amp;quot;); library(pacman)
p_load(&amp;quot;getTBinR&amp;quot;)
p_load(&amp;quot;ggplot2&amp;quot;)
p_load(&amp;quot;viridis&amp;quot;)
p_load(&amp;quot;dplyr&amp;quot;)
p_load(&amp;quot;forcats&amp;quot;)
p_load(&amp;quot;ggridges&amp;quot;)
p_load_gh(&amp;quot;thomasp85/patchwork&amp;quot;)

## Pull TB data and summarise TB incidence rates by region using the median
tb_sum &amp;lt;- summarise_tb_burden(metric = &amp;quot;e_inc_100k&amp;quot;,
                              stat = &amp;quot;median&amp;quot;,
                              compare_all_regions = TRUE,
                              samples = 1000)

## Plot the median and IQR for each region
sum &amp;lt;- tb_sum %&amp;gt;% 
  rename(Region = area) %&amp;gt;% 
  ggplot(aes(x = year, y = e_inc_100k, col = Region, fill = Region)) +
  geom_ribbon(alpha = 0.2, aes(ymin = e_inc_100k_lo, ymax = e_inc_100k_hi)) +
  scale_color_viridis(discrete = TRUE) +
  scale_fill_viridis(discrete = TRUE) +
  geom_line(alpha = 0.6, size = 1.2) +
  geom_point(size = 1.3) +
  theme_minimal() +
  facet_wrap(~Region, scales = &amp;quot;free_y&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(y = search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition,
       x = &amp;quot;Year&amp;quot;,
       title = &amp;quot;Regional Summary of Tuberculosis Incidence Rates - 2000 to 2016&amp;quot;,
       subtitle = &amp;quot;Median country level incidence rates (with 95% interquartile ranges) are shown&amp;quot;)

## Get the full TB burden dataset (including MDR TB)
tb &amp;lt;- get_tb_burden()

## Plot the distribution of country level TB incidence rates using ggridges
dist &amp;lt;- tb %&amp;gt;% 
  rename(Region = g_whoregion) %&amp;gt;% 
  mutate(year = year %&amp;gt;% 
           factor(ordered = TRUE) %&amp;gt;% 
           fct_rev) %&amp;gt;% 
  ggplot(aes(x = e_inc_100k, y = year, col = Region, fill = Region)) +
  geom_density_ridges(alpha = 0.6) +
  scale_color_viridis(discrete = TRUE) +
  scale_fill_viridis(discrete = TRUE) +
  theme_minimal() +
  facet_wrap(~Region, scales = &amp;quot;free_x&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(x = search_data_dict(&amp;quot;e_inc_100k&amp;quot;)$definition,
       y = &amp;quot;Year&amp;quot;,
       title = &amp;quot;Distribution of Country Level Tuberculosis Incidence Rates by Region - 2000 to 2016&amp;quot;,
       caption = &amp;quot;By @seabbs | Made with getTBinR | Source: World Health Organisation&amp;quot;)

## Map global TB incidence rates for 2016 using getTBinR
map &amp;lt;- map_tb_burden() +
  labs(caption = &amp;quot;&amp;quot;,
       title = &amp;quot;Map of Tuberculosis Incidence Rates - 2016&amp;quot;)

## Extract the top 10 high incidence countries in 2016.
high_inc_countries &amp;lt;- tb %&amp;gt;% 
  filter(year == 2016) %&amp;gt;% 
  arrange(desc(e_inc_100k)) %&amp;gt;% 
  slice(1:10) %&amp;gt;% 
  pull(country)

## Plot an overview of TB incidence rates in 2016.
high_inc_overview &amp;lt;- plot_tb_burden_overview(countries = high_inc_countries) +
  labs(caption = &amp;quot;&amp;quot;,
       title = &amp;quot;10 Countries with the Highest Tuberculosis Incidence Rates - 2016&amp;quot;) 

## Compose storyboard
storyboard &amp;lt;- (map + high_inc_overview) /
                 (sum | dist) +
                 plot_layout(heights = c(1, 2))

## Save storyboard
ggsave(&amp;quot;storyboard.png&amp;quot;,
       storyboard, width = 20, height = 15, dpi = 330)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./img/getTBinR/storyboard-5-4.png&#34; alt=&#34;getTBinR 0.5.4 storyboard&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For other examples of using &lt;code&gt;getTBinR&lt;/code&gt; to visualise the WHO TB data see &lt;a href=&#34;https://gist.github.com/seabbs&#34; target=&#34;_blank&#34;&gt;my&lt;/a&gt; gists, previous blog &lt;a href=&#34;https://www.samabbott.co.uk/tags/who/&#34; target=&#34;_blank&#34;&gt;posts&lt;/a&gt;, and the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;getTBinR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Tuberculosis Monitoring Indicators in England; Using Dimension Reduction and Clustering</title>
      <link>http://www.samabbott.co.uk/post/cluster-england-tb/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/cluster-england-tb/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I recently attended the &lt;a href=&#34;Public%20Health%20Research%20and%20Science%20Conference&#34;&gt;Public Health Research and Science Conference&lt;/a&gt;, run by &lt;a href=&#34;https://www.gov.uk/government/organisations/public-health-england&#34;&gt;Public Health England&lt;/a&gt; (PHE), at the University of Warwick. I was mainly there to present some work that I have been doing (along with my co-authors) estimating the direct effects of the 2005 change in BCG vaccination policy on Tuberculosis (TB) incidence rates (&lt;a href=&#34;https://www.samabbott.co.uk/talk/phe-applied-epi-2018/&#34;&gt;slides&lt;/a&gt;) but it was also a great opportunity to see what research is being done within, and partnered with, PHE. The standout out work for me was the nascent data science work that is being undertaken within PHE, which is currently focused around the &lt;a href=&#34;https://github.com/ropensci/fingertipsR&#34;&gt;&lt;code&gt;fingertipsR&lt;/code&gt;&lt;/a&gt; R package by Sebastian Fox. &lt;a href=&#34;https://github.com/ropensci/fingertipsR&#34;&gt;&lt;code&gt;fingertipsR&lt;/code&gt;&lt;/a&gt; provides an easy interface to access the &lt;a href=&#34;https://fingertips.phe.org.uk&#34;&gt;fingertips&lt;/a&gt; API, which contains data on a large variety of public health issues, and can be explored interactively online (see previous link).&lt;/p&gt;
&lt;p&gt;In this post we will focus on data on &lt;a href=&#34;https://en.wikipedia.org/wiki/Tuberculosis&#34;&gt;Tuberculosis&lt;/a&gt; (TB), which is predominately a respiratory disease and if left untreated kills approximately half of those infected. The majority of cases are symptom-less (known as latent TB), with 10% of latent cases progressing to active disease. It is thought that immediately after infection individuals are at a higher risk of proceeding to active disease, with the risk diminishing after several years. However, individuals who have carried the disease for many years can, and do, progress to active TB disease. This makes the control and management of TB on a population scale challenging as cases may either be due to recent transmission or be from the activation of latent cases who have carried the disease for many years. &lt;a href=&#34;https://en.wikipedia.org/wiki/HIV&#34;&gt;HIV&lt;/a&gt; infection is known to drastically increase the likelihood of progression to active TB disease and TB is the leading cause of death among people living with HIV (&lt;a href=&#34;http://www.who.int/hiv/topics/tb/tbhiv_facts_2015/en/&#34;&gt;source&lt;/a&gt;). Globally 10.4 million people fell ill with TB in 2016 alone, with 1.7 million deaths (&lt;a href=&#34;http://www.who.int/mediacentre/factsheets/fs104/en/&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In England, TB incidence rates have declined drastically over the course of the last century, with the introduction of BCG vaccination, effective TB treatments and improved standards of living. However, over the previous two decades incidence rates have remained relatively stable with incidence becoming increasing focused in at risk communities, such as the homeless population and the non-UK born living in high density urban areas (&lt;a href=&#34;https://www.gov.uk/government/publications/tuberculosis-in-england-annual-report&#34;&gt;source&lt;/a&gt;). Recently it has been recognised that TB interventions in England must be collaborative and consistent across the country (&lt;a href=&#34;http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(14)61638-X/fulltext&#34;&gt;source&lt;/a&gt;), as this has proven to be effective in other countries such as the USA.&lt;/p&gt;
&lt;p&gt;This post uses dimension reduction (&lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal Component Analysis&lt;/a&gt; (PCA)) and clustering (&lt;a href=&#34;https://en.wikipedia.org/wiki/K-medoids&#34;&gt;Partitioning Around Mediods&lt;/a&gt; (PAM)) to explore the Tuberculosis monitoring indicators available from the &lt;a href=&#34;https://fingertips.phe.org.uk&#34;&gt;fingertips&lt;/a&gt; API, using the &lt;a href=&#34;https://github.com/ropensci/fingertipsR&#34;&gt;&lt;code&gt;fingertipsR&lt;/code&gt;&lt;/a&gt; R package, for England. It aims to use hypothesis free techniques to generate clusters of counties with similar characteristics of TB indicators. This may help to identify regional variation in Tuberculosis monitoring indicators and possibly provide a framework for future improvements to TB control efforts. It also seeks to act as an example of dimension reduction and clustering analysis. Therefore, comments to improve this aspect of this post would be greatly appreciated (paper links, package recommendations, methodology improvements etc.)!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;p&gt;The first step is to load the packages required for the analysis, we do this using the fantastic &lt;code&gt;pacman&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!require(pacman)) install.packages(&amp;quot;pacman&amp;quot;); library(pacman)
p_load(&amp;quot;viridis&amp;quot;)
p_load(&amp;quot;broom&amp;quot;)
p_load(&amp;quot;knitr&amp;quot;)
p_load(&amp;quot;ggfortify&amp;quot;)
p_load(&amp;quot;gpclib&amp;quot;)
p_load(&amp;quot;rgdal&amp;quot;)
p_load(&amp;quot;raster&amp;quot;)
p_load(&amp;quot;maptools&amp;quot;)
p_load(&amp;quot;purrr&amp;quot;)
p_load(&amp;quot;FactoMineR&amp;quot;)
p_load(&amp;quot;cluster&amp;quot;)
p_load(&amp;quot;scales&amp;quot;)
p_load(&amp;quot;fingertipsR&amp;quot;)
p_load(&amp;quot;tidyverse&amp;quot;)
p_load(&amp;quot;knitr&amp;quot;)
p_load(&amp;quot;ggthemes&amp;quot;)
p_load_gh(&amp;quot;thomasp85/patchwork&amp;quot;, dependencies = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;As discussed in the introduction we are using &lt;a href=&#34;https://github.com/ropensci/fingertipsR&#34;&gt;&lt;code&gt;fingertipsR&lt;/code&gt;&lt;/a&gt; as our data source. The first step is to investigate the data profiles provided by the package which mention TB.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;profs &amp;lt;- profiles()

sel_profs &amp;lt;- profs[grepl(&amp;quot;TB&amp;quot;, profs$ProfileName),]

kable(sel_profs)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;ProfileID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ProfileName&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;DomainID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;DomainName&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938133208&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;LTBI programme monitoring&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We find that there are two profiles: key indicators and LTBI programme monitoring (screening the at-risk population for latent TB). We use the &lt;code&gt;indicators&lt;/code&gt; function to explore the variables available in each profile.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_inds &amp;lt;- indicators(ProfileID = sel_profs$ProfileID)

kable(tb_inds)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;IndicatorID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;IndicatorName&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;DomainID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;DomainName&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ProfileID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ProfileName&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91359&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB incidence in England&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91361&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB incidence (three year average)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91365&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of pulmonary TB cases that were culture confirmed&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91366&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of culture confirmed TB cases with drug susceptibility testing reported for the four first line agents&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91367&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of drug sensitive TB cases who had completed a full course of treatment by 12 months&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91368&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of drug sensitive TB cases who were lost to follow up at last reported outcome&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91369&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of drug sensitive TB cases who had died at last reported outcome&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91373&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of TB cases offered an HIV test&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91374&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of drug sensitive TB cases with at least one social risk factor who completed treatment within 12 months&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91375&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of culture confirmed TB cases with any first line drug resistance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91450&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of pulmonary TB cases starting treatment within two months of symptom onset&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;91451&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Proportion of pulmonary TB cases starting treatment within four months of symptom onset&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1938132814&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Key Indicators&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TB Strategy Monitoring Indicators&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This gives us 12 TB indicators, which we can now extract using the &lt;code&gt;fingertips_data&lt;/code&gt; function combined with a call to &lt;code&gt;purrr::map&lt;/code&gt;. This results in 12 tibbles, with the first being empty (TB incidence rates).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_df &amp;lt;- tb_inds$IndicatorID %&amp;gt;% map(~fingertips_data(IndicatorID = .))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key indicator, which we will match all of the remaining data too, is the three-year average TB incidence rates (as the annual TB incidence rate is missing and would also be more susceptible to noise). The following code extracts this at the county level, re-codes the value variable as recent incidence rates and pulls the overall incidence of cases. According to the &lt;a href=&#34;https://fingertips.phe.org.uk/profile/tb-monitoring&#34;&gt;fingertips&lt;/a&gt; website, local authorities and CCGs with fewer than 20 TB cases per year have had all data for the indicators (apart from three-year average TB incidence) suppressed to avoid deductive disclosure. We can therefore filter out these counties now to avoid issues with missing data later. We also adjust the time period to represent the final year for each rolling average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_inc &amp;lt;- tb_df[[2]] %&amp;gt;% 
  filter(AreaType %in% &amp;quot;County &amp;amp; UA&amp;quot;) %&amp;gt;% 
  dplyr::select(AreaName, Sex, Age, Timeperiod,
         rec_inc_rate = Value, rec_inc = Count) %&amp;gt;% 
  filter(rec_inc &amp;gt;= 20) %&amp;gt;% 
  mutate(Timeperiod = Timeperiod %&amp;gt;% 
           str_split(&amp;quot; - &amp;quot;) %&amp;gt;% 
           map_chr(first) %&amp;gt;% 
           as.numeric %&amp;gt;% 
           {. + 2} %&amp;gt;% 
           as.character) %&amp;gt;% 
  dplyr::select(-rec_inc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking through the other tibbles they all have the same structure - we can write a function using this knowledge to speed up data extraction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_df_extraction &amp;lt;- function(tb_df, var_name, area_type = &amp;quot;County &amp;amp; UA&amp;quot;) {
  df &amp;lt;- tb_df %&amp;gt;% 
    filter(AreaType %in% area_type) %&amp;gt;% 
    dplyr::select(AreaName, Sex, Age, Value, Timeperiod) %&amp;gt;% 
    rename_at(.vars = vars(Value), funs(paste0(var_name)))
  
  return(df)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now extract data for all remaining indicators, rename variables with meaningful names, join into a single tibble and then left join onto the TB incidence rate tibble. Data is only available aggregated for all ages and genders so we also drop these variables here. Finally we clean up Timeperiod into years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_names &amp;lt;- c(&amp;quot;prop_pul_cc&amp;quot;, &amp;quot;prop_cc_ds_front&amp;quot;, &amp;quot;prop_ds_treat_com_12&amp;quot;,
               &amp;quot;prop_ds_lost_to_follow&amp;quot;, &amp;quot;prop_ds_died&amp;quot;,
               &amp;quot;prop_tb_offered_hiv_test&amp;quot;, &amp;quot;prop_ds_rf_treat_com_12&amp;quot;,
               &amp;quot;prop_cc_dr_front&amp;quot;, &amp;quot;prop_p_start_treat_2_m_sym&amp;quot;,
               &amp;quot;prop_p_start_treat_4_m_sym&amp;quot;
)

extracted_tb &amp;lt;- map2(tb_df[-(1:2)], var_names, ~tb_df_extraction(.x, .y)) %&amp;gt;% 
  reduce(full_join, by = c(&amp;quot;AreaName&amp;quot;, &amp;quot;Sex&amp;quot;, &amp;quot;Age&amp;quot;, &amp;quot;Timeperiod&amp;quot;))

com_tb_df &amp;lt;- tb_inc %&amp;gt;% 
  left_join(extracted_tb, by = c(&amp;quot;AreaName&amp;quot;, &amp;quot;Sex&amp;quot;, &amp;quot;Age&amp;quot;, &amp;quot;Timeperiod&amp;quot;)) %&amp;gt;% 
  mutate(year = Timeperiod %&amp;gt;% as.numeric) %&amp;gt;% 
  mutate_if(is.character, as.factor) %&amp;gt;% 
  dplyr::select(-Timeperiod) %&amp;gt;% 
  filter(Sex %in% &amp;quot;Persons&amp;quot;, Age %in% &amp;quot;All ages&amp;quot;) %&amp;gt;% 
  dplyr::select(-Sex, -Age)

com_tb_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2,026 x 13
##    AreaName     rec_inc_rate prop_pul_cc prop_cc_ds_front prop_ds_treat_c…
##    &amp;lt;fct&amp;gt;               &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1 Hartlepool          10.0         NA                NA            NA    
##  2 Middlesbrou…        16.1         72.7             100.           82.6  
##  3 Stockton-on…         5.99        NA                NA            NA    
##  4 Blackburn w…        37.7         61.8             100.           80.7  
##  5 Blackpool            7.95        NA                NA            NA    
##  6 Kingston up…         5.06        NA                NA            NA    
##  7 East Riding…         3.28        NA                NA            NA    
##  8 Derby               19.3         66.7             100.           76.7  
##  9 Leicester           71.1         62.6             100.            0.546
## 10 Nottingham          16.9         80.0             100.           73.0  
## # ... with 2,016 more rows, and 8 more variables:
## #   prop_ds_lost_to_follow &amp;lt;dbl&amp;gt;, prop_ds_died &amp;lt;dbl&amp;gt;,
## #   prop_tb_offered_hiv_test &amp;lt;dbl&amp;gt;, prop_ds_rf_treat_com_12 &amp;lt;dbl&amp;gt;,
## #   prop_cc_dr_front &amp;lt;dbl&amp;gt;, prop_p_start_treat_2_m_sym &amp;lt;dbl&amp;gt;,
## #   prop_p_start_treat_4_m_sym &amp;lt;dbl&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need to check the completeness of the data. Ideally, we would use the most recent year of data for our clustering analysis but this may not be possible if variables are highly missing. The following function calculates the proportion of missing data in each year for each variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_frac_missing &amp;lt;- function(df) {
  df %&amp;gt;% 
    nest() %&amp;gt;% 
    mutate(missing = map(data,~map_dfr(. ,~sum(is.na(.))/length(.)))) %&amp;gt;% 
    dplyr::select(-data) %&amp;gt;% 
    unnest(missing) 
}

## Get the proportion missing per variableby year
tb_miss_per_year &amp;lt;- com_tb_df %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  get_frac_missing %&amp;gt;% 
  mutate_all(~round(., 2)) %&amp;gt;% 
  arrange(year) 

tb_miss_per_year %&amp;gt;% 
  mutate(year = as.character(year)) %&amp;gt;% 
  t %&amp;gt;%
  kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;year&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2002&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2003&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2004&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2005&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2006&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2007&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2008&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2009&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2010&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2012&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2014&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2015&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;AreaName&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rec_inc_rate&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_pul_cc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_cc_ds_front&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_ds_treat_com_12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_ds_lost_to_follow&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_ds_died&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.30&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_tb_offered_hiv_test&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_ds_rf_treat_com_12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_cc_dr_front&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_p_start_treat_2_m_sym&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_p_start_treat_4_m_sym&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.28&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.34&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that data completeness increases with time but that some variables are completely missing (e.g. &lt;code&gt;prop_ds_rf_treat_com_12&lt;/code&gt; and &lt;code&gt;prop_cc_dr_front&lt;/code&gt;). We therefore drop these variables and then identify which year has the lowest amount of missing data across all remaining variables (by looking at mean missingness).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Drop full missing variables
tb_partial_miss_year &amp;lt;- tb_miss_per_year %&amp;gt;% 
  select_if(~!sum(.) == length(.))

## Full missing variables
com_miss_vars &amp;lt;- setdiff(names(tb_miss_per_year), names(tb_partial_miss_year))

## Which year has the most complete data
tb_complete_years_all_vars &amp;lt;- com_tb_df %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  nest() %&amp;gt;% 
  mutate(missing = map(data,~mean(colSums(is.na(.))/nrow(.)))) %&amp;gt;% 
  dplyr::select(-data) %&amp;gt;% 
  unnest(missing) %&amp;gt;% 
  mutate(missing = round(missing, 2)) %&amp;gt;% 
  arrange(year)

kable(tb_complete_years_all_vars)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;year&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;missing&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2003&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2004&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2005&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2006&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2007&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2008&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2009&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2010&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2012&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2014&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2016&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The above table indicates that 2016 has a high proportion of missing data. From the previous table we saw that this was partially due to some variables being completely missing. The next best option is 2015 - this has a slightly higher proportion of missing data than previous years but has no variables that are completely missing and is the most relevant after 2016. The final question is to what extent missingness is still related to TB incidence rates. The following table investigates this by looking at what happens as counties are excluded using varying incidence rate cut-offs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;com_tb_df %&amp;gt;%
  filter(year == 2015) %&amp;gt;% 
  dplyr::select(-map_dbl(com_miss_vars, contains)) %&amp;gt;% 
  mutate(inc_rate_lower = list(seq(2, 20, 2))) %&amp;gt;% 
  unnest(inc_rate_lower) %&amp;gt;% 
  group_by(year, inc_rate_lower) %&amp;gt;% 
  filter(rec_inc_rate &amp;gt; inc_rate_lower) %&amp;gt;% 
  nest() %&amp;gt;% 
  mutate(missing = map(data,~mean(colSums(is.na(.))/nrow(.)))) %&amp;gt;% 
  dplyr::select(-data, -year) %&amp;gt;% 
  unnest(missing) %&amp;gt;% 
  kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;inc_rate_lower&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;missing&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3117647&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2598291&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1934066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1014085&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0387097&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0148148&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The choice of incidence rate cut-off is somewhat arbitrary. However, it appears that a cut-off of at least 10 (per 100,000) is sufficient to deal with the majority of missing data. This is also a sensible cut-off as it represents the World Health Organisations 2035 target for TB eradication (&lt;a href=&#34;http://www.who.int/tb/post2015_TBstrategy.pdf&#34;&gt;source&lt;/a&gt;). This means that our analysis will focus on counties that have relatively high incidence rates in comparison to the median in England. Using everything we have learnt about the quality of the indicator data we now identify the near final analysis dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_df_2015 &amp;lt;- com_tb_df %&amp;gt;% 
  dplyr::select(-map_dbl(com_miss_vars, contains)) %&amp;gt;% 
  filter(year == 2015) %&amp;gt;% 
  filter(rec_inc_rate &amp;gt; 10)

tb_df_2015 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 62 x 11
##    AreaName     rec_inc_rate prop_pul_cc prop_cc_ds_front prop_ds_treat_c…
##    &amp;lt;fct&amp;gt;               &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1 Blackburn w…         29.0        71.4            100.              87.5
##  2 Derby                14.0        72.7            100.              71.9
##  3 Leicester            41.8        81.2             97.6             78.4
##  4 Nottingham           17.1        82.1             97.1             80.4
##  5 Stoke-on-Tr…         12.1        66.7            100.              80.8
##  6 Bristol              20.6        53.1             95.1             79.0
##  7 Swindon              10.7        57.1            100.              75.0
##  8 Peterborough         23.2        66.7            100.              69.0
##  9 Luton                30.1        80.0            100.              82.4
## 10 Reading              34.7        64.3             95.0             97.0
## # ... with 52 more rows, and 6 more variables:
## #   prop_ds_lost_to_follow &amp;lt;dbl&amp;gt;, prop_ds_died &amp;lt;dbl&amp;gt;,
## #   prop_tb_offered_hiv_test &amp;lt;dbl&amp;gt;, prop_p_start_treat_2_m_sym &amp;lt;dbl&amp;gt;,
## #   prop_p_start_treat_4_m_sym &amp;lt;dbl&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final step is to deal with the remaining missing data. As all variables, except incidence rate, are missing for some counties we cannot reliably impute the data. We therefore drop it and make a note of the counties for which data was not available.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_clean_2015 &amp;lt;- tb_df_2015 %&amp;gt;% 
  drop_na() %&amp;gt;% 
  select(-year)

missing_regions &amp;lt;- setdiff(tb_df_2015$AreaName %&amp;gt;% as.character, tb_clean_2015$AreaName %&amp;gt;% as.character)

missing_regions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Wokingham&amp;quot; &amp;quot;Bedford&amp;quot;   &amp;quot;Bury&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This leave us with a tidy and complete dataset with data on TB monitoring indicators for 59 counties in 2015.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dimension-reduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dimension reduction&lt;/h2&gt;
&lt;p&gt;We are now ready to conduct some clustering analysis on this data. The first step is to reduce the dimensionality of the data using principal component analysis (PCA). We use the &lt;code&gt;estim_ncp&lt;/code&gt; function (which uses a method outlined in this &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0167947311004099&#34;&gt;paper&lt;/a&gt;) from the &lt;code&gt;FactoMineR&lt;/code&gt; package to estimate the number of principal components required. We then perform PCA (using &lt;code&gt;prcomp&lt;/code&gt;) and plot the variance explained by each component as a check on &lt;code&gt;estim_ncp&lt;/code&gt;. All of the following analysis is done using nested tibbles and so can be easily generalised to higher dimensional use cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_pca &amp;lt;- tb_clean_2015 %&amp;gt;% 
  nest() %&amp;gt;% 
  mutate(
    numeric_data = map(data, ~select_if(., is.numeric) %&amp;gt;% 
                         as.data.frame()),
    optimal_pca_no = map(numeric_data, ~estim_ncp(., 
                                                  scale = TRUE, 
                                                  ncp.min = 2, 
                                                  ncp.max = 10)) %&amp;gt;% 
      map_dbl(~.$ncp),
    pca = map(numeric_data, ~prcomp(.x, 
                                    center = TRUE, 
                                    scale = TRUE)),
    pca_data = map(pca, ~.$x),
    pca_aug = map2(pca, data, ~augment(.x, data = .y)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We find that the optimal number of principal components is 2. We can also plot the percentage of variance explained in order to evaluate this choice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Variance explained
var_exp &amp;lt;- tb_pca %&amp;gt;% 
  dplyr::select(-optimal_pca_no) %&amp;gt;% 
  unnest(pca_aug) %&amp;gt;% 
  summarize_at(.vars = vars(contains(&amp;quot;PC&amp;quot;)), .funs = funs(var)) %&amp;gt;% 
  gather(key = pc, value = variance) %&amp;gt;% 
  mutate(var_exp = variance/sum(variance) * 100,
         cum_var_exp = cumsum(var_exp),
         pc = str_replace(pc, &amp;quot;.fitted&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;% 
           str_replace(&amp;quot;PC&amp;quot;, &amp;quot;&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_exp %&amp;gt;% 
  rename(
    `Variance Explained` = var_exp,
    `Cumulative Variance Explained` = cum_var_exp
  ) %&amp;gt;% 
  gather(key = key, value = value, `Variance Explained`, `Cumulative Variance Explained`) %&amp;gt;%
  mutate(key = key %&amp;gt;% 
           factor(levels  = c(&amp;quot;Variance Explained&amp;quot;, 
                              &amp;quot;Cumulative Variance Explained&amp;quot;))) %&amp;gt;% 
  mutate(value = value / 100) %&amp;gt;% 
  mutate(pc = factor(pc, levels = as.character(1:max(var_exp$pc %&amp;gt;% as.numeric)))) %&amp;gt;% 
  ggplot(aes(pc, value, group = key)) + 
  geom_point(size = 2, alpha = 0.8) + 
  geom_line(size = 1.1, alpha = 0.6) + 
  facet_wrap(~key, scales = &amp;quot;free_y&amp;quot;) +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 1, 0.05), lim = c(0, NA),
                     minor_breaks = NULL, labels = percent) +
  labs(
    title = &amp;quot;Variance Explained by Principal Component&amp;quot;,
    subtitle = paste0(&amp;quot;The optimal number of principal components suggested by estim_ncp was &amp;quot;,
                      tb_pca$optimal_pca_no, &amp;quot; which explains &amp;quot;, round(var_exp$cum_var_exp[[2]], 0), &amp;quot;% of the data.&amp;quot;),
    x = &amp;quot;Principal Component&amp;quot;,
    y = &amp;quot;Variance Explained (%)&amp;quot;,
    caption = &amp;quot;@seabbs Source: Public Health England (fingertipsR)&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-03-28-cluster-england-tb_files/figure-html/plot-var-explained-1.png&#34; width=&#34;2640&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above plot shows that only 42% of the variance in the data is explained by the first two principle components (PCs) even though the &lt;code&gt;estim_ncp&lt;/code&gt; function suggested that this was the optimal number. This indicates that there is large amount of noise in the data with a large amount of non-systematic between county variation. Another approach, using the ‘elbow’ (change from decreasing to stable amount of variance explained), would estimate that 8 PCs are required to explain the variance in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;clustering-of-tb-monitoring-indicators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clustering of TB Monitoring Indicators&lt;/h2&gt;
&lt;p&gt;Next, we can now perform clustering using the partitioning around medoids algorithm on the first two principal components. This approach should be more stable than K means and also has the benefit of producing a metric (the average silhouette width) which can be used to estimate the number of clusters that provides the best fitting model. The outline of the PAM algorithm is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Randomly select k observations as the initial medoid.&lt;/li&gt;
&lt;li&gt;Assign each observation to the closest medoid.&lt;/li&gt;
&lt;li&gt;Swap each medoid and non-medoid observation, computing the dissimilarity cost.&lt;/li&gt;
&lt;li&gt;Select the configuration that minimizes the total dissimilarity.&lt;/li&gt;
&lt;li&gt;Repeat steps 2 through 4 until there is no change in the medoids.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Again, we use an approach that makes use of nested tibbles, this should be easier to generalise to other use cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Perform pam on pca data 1 to 10 groups
tb_pca_pam &amp;lt;- tb_pca %&amp;gt;%
  mutate(centers = list(2:10)) %&amp;gt;% 
  unnest(centers, .preserve = everything()) %&amp;gt;% 
  dplyr::select(-centers, centers = centers1) %&amp;gt;% 
  group_by(centers) %&amp;gt;% 
  mutate(
    pam = map(pca_data,
              ~ pam(x = .x[, 1:optimal_pca_no], k = centers, stand = TRUE)),
    clusters = map(pam, ~.$clustering),
    avg_silhouette_width = map(pam, ~.$silinfo$avg.width),
    data_with_clusters = map2(.x = data, .y = clusters, ~mutate(.x, cluster = factor(.y, ordered = TRUE)))
  ) %&amp;gt;% 
  ungroup

tb_pca_pam&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 11
##   optimal_pca_no data   numeric_data  pca   pca_data pca_aug centers pam  
##            &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt; &amp;lt;list&amp;gt;        &amp;lt;lis&amp;gt; &amp;lt;list&amp;gt;   &amp;lt;list&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;lis&amp;gt;
## 1             2. &amp;lt;tibb… &amp;lt;data.frame … &amp;lt;S3:… &amp;lt;dbl [5… &amp;lt;data.…       2 &amp;lt;S3:…
## 2             2. &amp;lt;tibb… &amp;lt;data.frame … &amp;lt;S3:… &amp;lt;dbl [5… &amp;lt;data.…       3 &amp;lt;S3:…
## 3             2. &amp;lt;tibb… &amp;lt;data.frame … &amp;lt;S3:… &amp;lt;dbl [5… &amp;lt;data.…       4 &amp;lt;S3:…
## 4             2. &amp;lt;tibb… &amp;lt;data.frame … &amp;lt;S3:… &amp;lt;dbl [5… &amp;lt;data.…       5 &amp;lt;S3:…
## 5             2. &amp;lt;tibb… &amp;lt;data.frame … &amp;lt;S3:… &amp;lt;dbl [5… &amp;lt;data.…       6 &amp;lt;S3:…
## 6             2. &amp;lt;tibb… &amp;lt;data.frame … &amp;lt;S3:… &amp;lt;dbl [5… &amp;lt;data.…       7 &amp;lt;S3:…
## 7             2. &amp;lt;tibb… &amp;lt;data.frame … &amp;lt;S3:… &amp;lt;dbl [5… &amp;lt;data.…       8 &amp;lt;S3:…
## 8             2. &amp;lt;tibb… &amp;lt;data.frame … &amp;lt;S3:… &amp;lt;dbl [5… &amp;lt;data.…       9 &amp;lt;S3:…
## 9             2. &amp;lt;tibb… &amp;lt;data.frame … &amp;lt;S3:… &amp;lt;dbl [5… &amp;lt;data.…      10 &amp;lt;S3:…
## # ... with 3 more variables: clusters &amp;lt;list&amp;gt;, avg_silhouette_width &amp;lt;list&amp;gt;,
## #   data_with_clusters &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To assess the optimal number of clusters we can plot the average silhouette width. This indicates that two clusters are optimal, although this estimate may not be that robust as the average silhouette width is low (0.38) with 6, 7, and 8 clusters also having average silhouette widths that are comparable. In general, we prefer the parsimonious model and therefore we will only investigate two clusters for the remainder of this post. &lt;em&gt;Note that this does not test for no clustering (i.e a single cluster).&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get max silhouette width
max_silhouette_width &amp;lt;- tb_pca_pam %&amp;gt;% 
  dplyr::select(centers, avg_silhouette_width) %&amp;gt;% 
  unnest(avg_silhouette_width) %&amp;gt;% 
  arrange(desc(avg_silhouette_width)) %&amp;gt;% 
  slice(1)
  
## Plot average silhouette width
tb_pca_pam %&amp;gt;% 
  dplyr::select(centers, avg_silhouette_width) %&amp;gt;% 
  unnest(avg_silhouette_width) %&amp;gt;% 
  ggplot(aes(x = centers, y = avg_silhouette_width)) +
  geom_line(size = 2, alpha = 0.4) +
  geom_point(size = 3, alpha = 0.8) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(1, 10, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(NA, NA), breaks = seq(0, 1, 0.01), minor_breaks = NULL) +
  labs(title = &amp;quot;Average Silhouette Width by Number of PAM Clusters&amp;quot;,
       subtitle = paste0(&amp;quot;The optimal number of clusters identifed by avg. silhouette width was &amp;quot;,
                      max_silhouette_width$centers,
                      &amp;quot; with an avg. silhouette width of &amp;quot;, 
                      round(max_silhouette_width$avg_silhouette_width, 2)
       ),
       x = &amp;quot;Clusters&amp;quot;,
       y = &amp;quot;Avg. Silhouette Width&amp;quot;,
       caption = &amp;quot;@seabbs Source: Public Health England (fingertipsR)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-03-28-cluster-england-tb_files/figure-html/avg-silhouette-1.png&#34; width=&#34;2640&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;We can now explore the clusters we have identified. A useful way to do this is to visual the first two principal components overlaid with the original variable loadings, and the clusters we have identified.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot clusters
pca_plot &amp;lt;- tb_pca_pam %&amp;gt;% 
  filter(centers == max_silhouette_width$centers) %&amp;gt;% 
  select(data_with_clusters, pca) %&amp;gt;% 
  mutate(pca_graph = map2(.x = pca, 
                          .y = data_with_clusters,
                          ~ autoplot(.x, x = 1, y = 2, 
                                     loadings = TRUE, loadings.label = TRUE,
                                     loadings.label.repel = TRUE,
                                     loadings.label.size = 2, loadings.alpha = 0.8,
                                     loadings.label.vjust = -1, data = .y, 
                                     label = TRUE, label.label = &amp;quot;AreaName&amp;quot;,
                                     label.size = 1.5, label.vjust = -1, 
                                     alpha = 0.3, frame = TRUE, 
                                     frame.type = &amp;#39;convex&amp;#39;, frame.alpha= 0.05,
                                     colour = &amp;quot;cluster&amp;quot;, size = &amp;quot;rec_inc_rate&amp;quot;) +
                            theme_minimal() +
                            labs(x = paste0(&amp;quot;Principal Component 1 (Variance Explained: &amp;quot;,
                                            round(var_exp$var_exp[[1]], 1), &amp;quot;%)&amp;quot;),
                                 y = paste0(&amp;quot;Principal Component 2 (Variance Explained: &amp;quot;,
                                            round(var_exp$var_exp[[2]], 1), &amp;quot;%)&amp;quot;)) +
                            guides(colour=guide_legend(title = &amp;quot;Cluster&amp;quot;, ncol = 2), 
                                   fill=guide_legend(title= &amp;quot;Cluster&amp;quot;, ncol = 2),
                                   size = guide_legend(title = &amp;quot;TB Incidence Rate (per 100,000 population)&amp;quot;,
                                                       ncol = 2)) +
                            scale_colour_viridis(option = &amp;quot;viridis&amp;quot;, 
                                                 discrete = TRUE, end = 0.5) +
                            scale_fill_viridis(option = &amp;quot;viridis&amp;quot;, 
                                               discrete = TRUE, end = 0.5) +
                            theme(legend.position = &amp;quot;bottom&amp;quot;, 
                                  legend.box = &amp;quot;horizontal&amp;quot;) +
                            labs(
                              title = &amp;quot;Tuberculosis Data in England; First Two Principal Components&amp;quot;,
                              subtitle = &amp;quot;The arrows are variable loadings and points are counties coloured by cluster membership&amp;quot;,
                              caption = &amp;quot;@seabbs Source: Public Health England (fingertipsR)&amp;quot;
                            )
  )) %&amp;gt;% 
  pull(pca_graph) %&amp;gt;% 
  first


pca_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-03-28-cluster-england-tb_files/figure-html/plot-pca-1.png&#34; width=&#34;2640&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this we see that the clusters are generally split by incidence rates with lower incidence rate counties also having a higher proportion that either die or are lost to follow up. The higher incidence counties have a higher proportion of cultured confirmed pulmonary cases and more cases that complete treatment within 12 months. It appears that the proportion of cases that start treatment within 2 and 4 months varies over both clusters. We can also see that the proportion lost to follow up is inversely related to the proportion that are offered HIV tests, with a higher proportion that are lost to follow up corresponding to a reduced proportion of cases being offered HIV tests.&lt;/p&gt;
&lt;p&gt;Another way of summarising the between cluster differences is to summarise the data by cluster, which is presented in the following plot. This approach to exploring differences between clusters may not be applicable to data with a large number of clusters, for this a faceted ridge plot (&lt;a href=&#34;https://github.com/clauswilke/ggridges&#34;&gt;&lt;code&gt;ggridges&lt;/code&gt;&lt;/a&gt;) would probably offer a better solution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_tb_df &amp;lt;- tb_pca_pam %&amp;gt;% 
  filter(centers == max_silhouette_width$centers) %&amp;gt;% 
  pull(data_with_clusters) %&amp;gt;% 
  map(~ gather(., key = &amp;quot;Variable&amp;quot;, value = &amp;quot;value&amp;quot;, -AreaName, -cluster)) %&amp;gt;% 
  first %&amp;gt;% 
  rename(Cluster = cluster) 

plot_cluster_diff &amp;lt;- sum_tb_df %&amp;gt;% 
  ggplot(aes(x = Variable, y = value, col = Cluster, fill = Cluster)) +
  geom_violin(draw_quantiles = c(0.025, 0.5, 0.975), alpha = 0.2, scale = &amp;quot;width&amp;quot;) +
  geom_jitter(position = position_jitterdodge(), alpha = 0.3) +
  coord_flip() +
  theme_minimal() +
  theme(legend.position = &amp;quot;bottom&amp;quot;) +
  scale_y_continuous(breaks = seq(0, 100, 5), minor_breaks = NULL) +
  scale_colour_viridis(option = &amp;quot;viridis&amp;quot;, discrete = TRUE, end = 0.5) +
  scale_fill_viridis(option = &amp;quot;viridis&amp;quot;, discrete = TRUE, end = 0.5) +
  labs( 
    title = &amp;quot;Tuberculosis in England; Summarised by Cluster&amp;quot;,
    subtitle = &amp;quot;Violin plots are scaled by width, with the 2.5%, 50% and 97.5% quantiles shown.&amp;quot;,
    x = &amp;quot;Variable&amp;quot;,
    y = &amp;quot;Incidence rate (per 100,000) for rec_int_rate, otherwise proportion (0-100%)&amp;quot;,
    caption = &amp;quot;@seabbs Source: Public Health England (fingertipsR)&amp;quot;)

plot_cluster_diff&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-03-28-cluster-england-tb_files/figure-html/summary-plot-1.png&#34; width=&#34;3960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To explore this further we can calculate the percentage difference between clusters for several of the variables’ summary statistics. The following table does this for the mean, the median, the 2.5% quantile, and the 97.5% quantile.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_tb_df %&amp;gt;% 
  group_by(Cluster, Variable) %&amp;gt;% 
  summarise(mean = mean(value), median = median(value),
            lll = quantile(value, 0.025),
            hhh = quantile(value, 0.975)) %&amp;gt;% 
  group_by(Variable) %&amp;gt;% 
  mutate_if(is.numeric, .funs = funs((lag(.) - .)/ .)) %&amp;gt;% 
  drop_na %&amp;gt;% 
  mutate_if(is.numeric, .funs = funs(paste0(round(. * 100, 1), &amp;quot;%&amp;quot;))) %&amp;gt;% 
  ungroup %&amp;gt;% 
  mutate(Variable = factor(Variable, levels = rev(.$Variable))) %&amp;gt;% 
  arrange(Variable) %&amp;gt;% 
  rename(Mean = mean, Median = median, `2.5% Quantile` = lll, `97.5% Quantile` = hhh) %&amp;gt;% 
  select(-Cluster) %&amp;gt;% 
  kable&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Variable&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Median&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;2.5% Quantile&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;97.5% Quantile&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rec_inc_rate&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;46.2%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;26.5%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-7.6%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;123.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_tb_offered_hiv_test&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2.4%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.5%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2.6%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_pul_cc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;16.2%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15.4%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32.5%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;17.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_p_start_treat_4_m_sym&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;11.8%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;13.3%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;28.5%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;9.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_p_start_treat_2_m_sym&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12.1%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;9.2%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-28.2%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;21%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_ds_treat_com_12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;11.1%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;11.1%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;11.8%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_ds_died&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-56%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-51.6%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-100%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;-61.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;prop_cc_ds_front&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.1%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.6%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From the plot and table above, we see that, cluster 1 contains the counties with higher incidence rates and therefore has a higher median incidence rate. At the same time, it has a large interquartile range but a similar lower quartile to cluster 2. We also see that in cluster 1 a greater number of counties are not offering HIV tests to all cases, although it appears the majority of counties are offering HIV tests to 100% of TB cases in both cases. In cluster 2 there is a large reduction in the proportion of pulmonary cases that were culture confirmed. In addition, in pulmonary cases, fewer start treatment within 2 or 4 months of developing symptoms in comparison to cluster 1. There is also a moderate increase in the proportion of cases that are lost to follow up and a large increase in the proportion of cases that died in cluster 2 compared to cluster 1. There is little difference between clusters for the proportion of cultured confirmed cases which had drug susceptibility reported, with the majority of clusters reporting on 100% of cases.&lt;/p&gt;
&lt;p&gt;A more visual way to understand the clustering of TB in England based on the data we have extracted using the &lt;code&gt;fingertipsR&lt;/code&gt; package is to plot the cluster membership for each county on a map. We do this using data on the &lt;a href=&#34;https://borders.ukdataservice.ac.uk/easy_download_data.html?data=England_ct_2011&#34;&gt;outlines of the counties in England&lt;/a&gt; from the UK data service (&lt;a href=&#34;https://twitter.com/statsepi&#34;&gt;Darren L Dahly&lt;/a&gt; kindly pointed me in the direction of this data - thank you!)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Make the plot into a function as ggplot2 object is very large and cause git issues (i.e therefore easier to remake the plot than it is to transfer between code chunks)
tb_cluster_map &amp;lt;- function(tb_pca_pam) {
  ## Some issues here with extracting code from the sp file
## Solved using the folling maptools functions - improvements appreciated!
gpclibPermit()

england_counties &amp;lt;- shapefile(&amp;quot;../../static/data/shapefiles/england-2011-ct-shape/england_ct_2011.shp&amp;quot;) %&amp;gt;%
  fortify(region = &amp;quot;code&amp;quot;) %&amp;gt;% 
  as_tibble

england_urban_areas &amp;lt;- shapefile(&amp;quot;../../static/data/shapefiles/england-urb-2001-shape/england_urb_2001.shp&amp;quot;) %&amp;gt;% 
  fortify(region = &amp;quot;name&amp;quot;) %&amp;gt;% 
  as_tibble %&amp;gt;% 
  filter(id %in% c(&amp;quot;Greater London Urban Area&amp;quot;, 
                   &amp;quot;Greater Manchester Urban Area&amp;quot;,
                   &amp;quot;Bristol Urban Area&amp;quot;,
                   &amp;quot;West Midlands Urban Area&amp;quot;,
                   &amp;quot;Milton Keynes Urban Area&amp;quot;))

## Make custom positions for urban area labels
urban_area_labels &amp;lt;- england_urban_areas %&amp;gt;%
  group_by(id) %&amp;gt;% 
  slice(100) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(long = long - 200000,
         lat = lat + 20000)
  
  
tb_cluster_results &amp;lt;- tb_pca_pam %&amp;gt;% 
  filter(centers == max_silhouette_width$centers) %&amp;gt;% 
  pull(data_with_clusters) %&amp;gt;% 
  first

tb_cluster_results &amp;lt;- tb_df[[2]] %&amp;gt;% 
              dplyr::select(AreaName, AreaCode, AreaType) %&amp;gt;% 
  filter(AreaType %in% &amp;quot;County &amp;amp; UA&amp;quot;) %&amp;gt;% 
              unique %&amp;gt;% 
  left_join(tb_cluster_results,
            by = &amp;quot;AreaName&amp;quot;) %&amp;gt;% 
  left_join(england_counties, by = c(&amp;quot;AreaCode&amp;quot; = &amp;quot;id&amp;quot;))

   tb_cluster_results %&amp;gt;% 
  rename(Cluster = cluster) %&amp;gt;% 
  drop_na(Cluster) %&amp;gt;% 
  dplyr::select(long, lat, Cluster, group) %&amp;gt;% 
  ggplot( 
                 aes(x = long, 
                     y = lat,
                     fill = Cluster)) +
    geom_polygon(data = england_urban_areas, 
                 aes(group = group, fill = NULL),
                 alpha = 0.4) +
    geom_path(data = tb_cluster_results, 
              aes(group = group, fill = NULL), 
              alpha = 0.4) +
    geom_polygon(data = tb_cluster_results, 
                 aes(group = group, fill = NULL),
                 alpha = 0.1) +
    geom_polygon(aes(group = group), alpha = 0.6) +
    geom_line(data = urban_area_labels %&amp;gt;% 
                bind_rows(urban_area_labels %&amp;gt;% 
                            mutate(long = long + 200000, 
                                   lat = lat - 20000)),
              aes(fill = NA, group = id), alpha = 0.8) + 
    geom_label(data = urban_area_labels,
              aes(label = id), fill = &amp;quot;grey&amp;quot;) +
    scale_fill_viridis(option = &amp;quot;viridis&amp;quot;, discrete = TRUE,
                       end = 0.5) +
    coord_equal() +
    theme_map() +
    theme(legend.position = &amp;quot;bottom&amp;quot;) +
    labs(title = &amp;quot;Tuberculosis Monitoriing Indicators; Map of County Level Clusters in England&amp;quot;,
         subtitle = &amp;quot;Using data from 2015 - only counties with incidence rates above 10 per 100,000 population and complete data are shown&amp;quot;,
         caption = &amp;quot;Selected urban areas are shown (dark grey) and labelled.
@seabbs Source: Public Health England (fingertipsR)
Contains National Statistics data © Crown copyright and database right 2018. 
         Contains OS data © Crown copyright and database right 2018&amp;quot;)
}

plot_tb_cluster_map &amp;lt;- tb_cluster_map(tb_pca_pam)

ggsave(&amp;quot;../../static/img/fingertips/map-tb-clust.png&amp;quot;,
       plot_tb_cluster_map, width = 8, height = 8, dpi = 330)

plot_tb_cluster_map&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-03-28-cluster-england-tb_files/figure-html/map-cluster-membership-1.png&#34; width=&#34;2640&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Remove objects to reduce stored chunk size
rm(&amp;quot;plot_tb_cluster_map&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the map above we see that cluster 1 is mainly made up of counties in London, Birmingham, and in the North West of England. Cluster 2 accounts for the majority of counties that are not within these large urban areas, as well the remaining as several counties in Birmingham and the North West and a single county in London. As expected we see that the majority of counties in England have been excluded from our analysis due to low incidence rates or missing data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-and-wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary and Wrap-up&lt;/h2&gt;
&lt;p&gt;In this post we have explored the &lt;code&gt;fingertipsR&lt;/code&gt; R package and the data on TB monitoring indicators for counties in England that is provided through the &lt;code&gt;fingertips&lt;/code&gt; API. We found that there was a large amount of non-random missing data, much of which was due to censoring to prevent deductive disclosure. However, several variables were entirely missing and even once counties with low incidence rates were removed there was still several counties with little TB monitoring data available. Substantial improvements could be made here to improve any future analysis or monitoring using this dataset.&lt;/p&gt;
&lt;p&gt;Once counties and variables with missing data had been removed we found that there was substantial non-systematic variation between counties with only 42% of variation explained by the optimal number of principal components. The large amount of variation between counties we observed reinforces the need for the collaborative TB strategy in England which was launched in 2015. Hopefully once data becomes available for 2016, and 2017 this will show a decrease in variation between counties.&lt;/p&gt;
&lt;p&gt;We found that, after dimension reduction, the data best supported two clusters. One cluster contained the majority of high incidence counties, which also had a higher proportion of culture confirmed pulmonary TB cases and more cases completing treatment in the first 12 months. This cluster was mainly centred around the greater London area but also included counties in Birmingham and the North West. The second cluster contained mainly counties not in these large urban areas but did contain the remaining counties in the North West and Birmingham area, as well as a single county in London. This cluster had a lower proportion of cases that started treatment within 2 and 4 months of developing symptoms, as well as having a greater proportion of cases that were lost to follow up and that died.&lt;/p&gt;
&lt;p&gt;This analysis was limited by the TB monitoring indicators available, the large amount of missing data for the variables that were available, and the lack of high quality up to date data. There was a large amount of non-systematic variation present that was exluded from our clustering analysis and the clusters that we did identify cannot be considered to be highly robust. We did not test for a single cluster, which may have produced a model more consistent with the date. A single clustering algorithm was used, and although it is considered a robust approach, further validation of these results is necessary using another (or multiple) clustering methods.&lt;/p&gt;
&lt;p&gt;Hopefully this post proved useful for informing you about the present state of TB in England, but also as an example of dimension reduction and clustering analysis. This analysis serves as a framework for a similar analysis that I am carrying out as part of my PhD so any comments, suggested improvements etc. would be greatly appreciated.&lt;/p&gt;
&lt;p&gt;To wrap-up this post we use the &lt;code&gt;patchwork&lt;/code&gt; package to quickly compose a storyboard of the results from the clustering analysis carried out above. See &lt;a href=&#34;https://www.samabbott.co.uk/post/2018-03-28-cluster-england-tb_files/figure-html/storyboard-1.png&#34;&gt;here&lt;/a&gt; for a full size version.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_cluster_map &amp;lt;- tb_cluster_map(tb_pca_pam)

storyboard &amp;lt;- plot_tb_cluster_map | ( pca_plot / plot_cluster_diff)

ggsave(&amp;quot;../../static/img/fingertips/storyboard-fingertips-tb-clust.png&amp;quot;,
       storyboard, width = 20, height = 16, dpi = 330)

storyboard&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-03-28-cluster-england-tb_files/figure-html/storyboard-1.png&#34; width=&#34;6600&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Remove to reduce chunk size as over 100mb git limit
rm(&amp;quot;storyboard&amp;quot;, &amp;quot;plot_tb_cluster_map&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating the effect of the 2005 UK BCG vaccination policy change: A retrospective cohort study using the Enhanced Tuberculosis Surveillance system, 2000-2015</title>
      <link>http://www.samabbott.co.uk/talk/phe-applied-epi-2018/</link>
      <pubDate>Tue, 20 Mar 2018 11:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/talk/phe-applied-epi-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introduction to getTBinR</title>
      <link>http://www.samabbott.co.uk/talk/bidd_group_talk_18_03_06/</link>
      <pubDate>Tue, 06 Mar 2018 14:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/talk/bidd_group_talk_18_03_06/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploring Estimates of the Tuberculosis Case Fatality Ratio - with getTBinR</title>
      <link>http://www.samabbott.co.uk/post/est-cfr-gettbinr/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/est-cfr-gettbinr/</guid>
      <description>


&lt;p&gt;This is a quick post exploring estimates of the case fatality ratio for Tuberculosis (TB) from &lt;a href=&#34;http://www.who.int/tb/country/data/download/en/&#34;&gt;data&lt;/a&gt; published by the World Health Organisation (WHO). It makes use of &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR&#34;&gt;getTBinR&lt;/a&gt; (which is now on &lt;a href=&#34;https://cran.r-project.org/web/packages/getTBinR/index.html&#34;&gt;CRAN&lt;/a&gt;), &lt;a href=&#34;https://github.com/trinker/pacman&#34;&gt;pacman&lt;/a&gt; for package management, &lt;a href=&#34;https://github.com/hrbrmstr/hrbrthemes&#34;&gt;hrbrthemes&lt;/a&gt; for plot themes, and &lt;a href=&#34;https://github.com/thomasp85/patchwork&#34;&gt;pathwork&lt;/a&gt; for combining multiple plots into a storyboard. For an introduction to using getTBinR to explore the WHO TB data see &lt;a href=&#34;https://www.samabbott.co.uk/post/intro-gettbinr/&#34;&gt;this&lt;/a&gt; post.&lt;/p&gt;
&lt;p&gt;It is estimated that in 2016 there was more than 10 million cases of active TB, with 1.3 million deaths (&lt;a href=&#34;https://en.wikipedia.org/wiki/Tuberculosis&#34;&gt;source&lt;/a&gt;). This makes TB the leading cause of death from an infectious disease. Despite the high numbers of cases globally, and high levels of mortality, TB is often considered a historic disease in developed countries. However, in recent years previously declining trends have flattened out and there is increasing concern surrounding multi-drug resistant TB, which has a greatly reduced the rate of successful treatment (&lt;a href=&#34;http://www.who.int/mediacentre/factsheets/fs104/en/&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;A key component of TB control is reducing the case fatality rate from active TB, this post explores WHO estimates for the TB case fatality rate in 2016 and then estimates the case fatality rates in previous years. The first step is to download and install the required packages, &lt;code&gt;pacman&lt;/code&gt; is used to manage this process.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!require(pacman)) install.packages(&amp;quot;pacman&amp;quot;); library(pacman)
p_load(&amp;quot;getTBinR&amp;quot;)
p_load(&amp;quot;tidyverse&amp;quot;)
p_load(&amp;quot;viridis&amp;quot;)
p_load(&amp;quot;hrbrthemes&amp;quot;)
p_load(&amp;quot;scales&amp;quot;)
p_load_gh(&amp;quot;thomasp85/patchwork&amp;quot;, dependencies = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we use &lt;code&gt;getTBinR&lt;/code&gt; to download the TB burden data and its accompanying data dictionary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_burden &amp;lt;- get_tb_burden(verbose = FALSE)
dict &amp;lt;- get_data_dict(verbose = FALSE)

tb_burden&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3,651 x 71
##    country  iso2  iso3  iso_numeric g_whoregion  year e_pop_num e_inc_100k
##    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 Afghani… AF    AFG             4 Eastern Me…  2000  20093756       190.
##  2 Afghani… AF    AFG             4 Eastern Me…  2001  20966463       189.
##  3 Afghani… AF    AFG             4 Eastern Me…  2002  21979923       189.
##  4 Afghani… AF    AFG             4 Eastern Me…  2003  23064851       189.
##  5 Afghani… AF    AFG             4 Eastern Me…  2004  24118979       189.
##  6 Afghani… AF    AFG             4 Eastern Me…  2005  25070798       189.
##  7 Afghani… AF    AFG             4 Eastern Me…  2006  25893450       189.
##  8 Afghani… AF    AFG             4 Eastern Me…  2007  26616792       189.
##  9 Afghani… AF    AFG             4 Eastern Me…  2008  27294031       189.
## 10 Afghani… AF    AFG             4 Eastern Me…  2009  28004331       189.
## # ... with 3,641 more rows, and 63 more variables: e_inc_100k_lo &amp;lt;dbl&amp;gt;,
## #   e_inc_100k_hi &amp;lt;dbl&amp;gt;, e_inc_num &amp;lt;int&amp;gt;, e_inc_num_lo &amp;lt;int&amp;gt;,
## #   e_inc_num_hi &amp;lt;int&amp;gt;, e_inc_num_f014 &amp;lt;int&amp;gt;, e_inc_num_f014_lo &amp;lt;int&amp;gt;,
## #   e_inc_num_f014_hi &amp;lt;int&amp;gt;, e_inc_num_f15plus &amp;lt;int&amp;gt;,
## #   e_inc_num_f15plus_lo &amp;lt;int&amp;gt;, e_inc_num_f15plus_hi &amp;lt;int&amp;gt;,
## #   e_inc_num_f &amp;lt;int&amp;gt;, e_inc_num_f_lo &amp;lt;int&amp;gt;, e_inc_num_f_hi &amp;lt;int&amp;gt;,
## #   e_inc_num_m014 &amp;lt;int&amp;gt;, e_inc_num_m014_lo &amp;lt;int&amp;gt;,
## #   e_inc_num_m014_hi &amp;lt;int&amp;gt;, e_inc_num_m15plus &amp;lt;int&amp;gt;,
## #   e_inc_num_m15plus_lo &amp;lt;int&amp;gt;, e_inc_num_m15plus_hi &amp;lt;int&amp;gt;,
## #   e_inc_num_m &amp;lt;int&amp;gt;, e_inc_num_m_lo &amp;lt;int&amp;gt;, e_inc_num_m_hi &amp;lt;int&amp;gt;,
## #   e_inc_num_014 &amp;lt;int&amp;gt;, e_inc_num_014_lo &amp;lt;int&amp;gt;, e_inc_num_014_hi &amp;lt;int&amp;gt;,
## #   e_inc_num_15plus &amp;lt;int&amp;gt;, e_inc_num_15plus_lo &amp;lt;int&amp;gt;,
## #   e_inc_num_15plus_hi &amp;lt;int&amp;gt;, e_tbhiv_prct &amp;lt;dbl&amp;gt;, e_tbhiv_prct_lo &amp;lt;dbl&amp;gt;,
## #   e_tbhiv_prct_hi &amp;lt;dbl&amp;gt;, e_inc_tbhiv_100k &amp;lt;dbl&amp;gt;,
## #   e_inc_tbhiv_100k_lo &amp;lt;dbl&amp;gt;, e_inc_tbhiv_100k_hi &amp;lt;dbl&amp;gt;,
## #   e_inc_tbhiv_num &amp;lt;int&amp;gt;, e_inc_tbhiv_num_lo &amp;lt;int&amp;gt;,
## #   e_inc_tbhiv_num_hi &amp;lt;int&amp;gt;, e_mort_exc_tbhiv_100k &amp;lt;dbl&amp;gt;,
## #   e_mort_exc_tbhiv_100k_lo &amp;lt;dbl&amp;gt;, e_mort_exc_tbhiv_100k_hi &amp;lt;dbl&amp;gt;,
## #   e_mort_exc_tbhiv_num &amp;lt;int&amp;gt;, e_mort_exc_tbhiv_num_lo &amp;lt;int&amp;gt;,
## #   e_mort_exc_tbhiv_num_hi &amp;lt;int&amp;gt;, e_mort_tbhiv_100k &amp;lt;dbl&amp;gt;,
## #   e_mort_tbhiv_100k_lo &amp;lt;dbl&amp;gt;, e_mort_tbhiv_100k_hi &amp;lt;dbl&amp;gt;,
## #   e_mort_tbhiv_num &amp;lt;int&amp;gt;, e_mort_tbhiv_num_lo &amp;lt;int&amp;gt;,
## #   e_mort_tbhiv_num_hi &amp;lt;int&amp;gt;, e_mort_100k &amp;lt;dbl&amp;gt;, e_mort_100k_lo &amp;lt;dbl&amp;gt;,
## #   e_mort_100k_hi &amp;lt;dbl&amp;gt;, e_mort_num &amp;lt;int&amp;gt;, e_mort_num_lo &amp;lt;int&amp;gt;,
## #   e_mort_num_hi &amp;lt;int&amp;gt;, cfr &amp;lt;dbl&amp;gt;, cfr_lo &amp;lt;dbl&amp;gt;, cfr_hi &amp;lt;dbl&amp;gt;,
## #   c_newinc_100k &amp;lt;dbl&amp;gt;, c_cdr &amp;lt;dbl&amp;gt;, c_cdr_lo &amp;lt;dbl&amp;gt;, c_cdr_hi &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 402 x 4
##    variable_name            dataset              code_list   definition   
##    &amp;lt;chr&amp;gt;                    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;        
##  1 bmu                      Community engagement &amp;quot;&amp;quot;          Number of TB…
##  2 bmu_community_impl       Community engagement &amp;quot;&amp;quot;          Number of TB…
##  3 bmu_ref_data             Community engagement &amp;quot;&amp;quot;          Number of Ba…
##  4 bmu_rxsupport_data       Community engagement &amp;quot;&amp;quot;          Number of Ba…
##  5 bmu_rxsupport_data_coh   Community engagement &amp;quot;&amp;quot;          Total number…
##  6 community_data_available Community engagement 0=No; 1=Yes Are data ava…
##  7 notified_ref             Community engagement &amp;quot;&amp;quot;          Total number…
##  8 notified_ref_community   Community engagement &amp;quot;&amp;quot;          Total number…
##  9 rxsupport_community_coh  Community engagement &amp;quot;&amp;quot;          Total number…
## 10 rxsupport_community_succ Community engagement &amp;quot;&amp;quot;          Total number…
## # ... with 392 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to explore the case fatality rate (&lt;code&gt;cfr&lt;/code&gt;), lets first look at the WHO definition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;search_data_dict(&amp;quot;cfr&amp;quot;, verbose = FALSE) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;variable_name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;dataset&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;code_list&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;definition&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cfr&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated TB case fatality ratio&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Maps can be a useful first visualisation for summarising spatial data, although they can also be misleading for more complex comparisons. Lets look at global TB incidence rates (per 100,000 population) in 2016, and the WHO estimate of the case fatality ratio in the same year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Map TB incidence rates - 2016
mp1 &amp;lt;- map_tb_burden(year = 2016, verbose = FALSE) +
  scale_fill_viridis(option = &amp;quot;cividis&amp;quot;, direction = -1) +
  labs(title = &amp;quot;Map of Tuberculosis Incidence Rates - 2016&amp;quot;,
       subtitle = &amp;quot;Incidence rates are per 100,000 population&amp;quot;)

mp1 +
  labs(caption = &amp;quot;@seabbs Source: WHO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-02-27-estimate-case-fatality-rate_files/figure-html/make-inc-map-1.png&#34; width=&#34;2310&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Map TB case fatality ratio -2016
mp2 &amp;lt;- map_tb_burden(metric = &amp;quot;cfr&amp;quot;, year = 2016, verbose= FALSE) +
  scale_fill_viridis(option = &amp;quot;cividis&amp;quot;, direction = -1) +
  labs(title = &amp;quot;Map of Tuberculosis Case Fatality Ratio - 2016&amp;quot;,
       subtitle = &amp;quot;Case fatality rate estimated by the WHO&amp;quot;)

mp2 +
  labs(caption = &amp;quot;@seabbs Source: WHO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-02-27-estimate-case-fatality-rate_files/figure-html/make-cfr-map-1.png&#34; width=&#34;2310&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Whilst quantitative insights are hard to glean from the above maps we can see that incidence rates appear to be highest in Africa and Asia. It also looks like the case fatality ratios are highest in these regions as well.&lt;/p&gt;
&lt;p&gt;The WHO have only provided estimates for the case fatality ratio for 2016, estimating the case fatality ratio for other years will allow trends over time to be explored. We do this by first summarising TB incidence rates, and mortality rates by region. This allows insights into the general global picture to be more easily extracted. The first step is to write a function to summarise rates on regional levels, we do this using &lt;code&gt;dplyr::summarise_at&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Make function to summarise rates in a given region
rate_region &amp;lt;- function(df = NULL, metric = NULL) {
  
  metric_vars &amp;lt;- c(metric, paste0(metric, &amp;quot;_lo&amp;quot;), paste0(metric, &amp;quot;_hi&amp;quot;))
  
  df &amp;lt;- df %&amp;gt;% 
    group_by(year, g_whoregion) %&amp;gt;% 
    summarise_at(.vars = c(metric_vars, &amp;quot;e_pop_num&amp;quot;),
                 .funs = funs(sum(as.numeric(.), na.rm = T))) %&amp;gt;% 
    ungroup() %&amp;gt;% 
    mutate_at(.vars = metric_vars,
              .funs = funs(. / e_pop_num * 1e5))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then write a reusable plotting function to visualise this data (making use of &lt;code&gt;aes_string&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plotting function for rates
plot_rate_region &amp;lt;- function(df = NULL, metric = NULL, title = NULL, subtitle = NULL, 
                             y_lab = NULL, scales = NULL) {
  
  metric_vars &amp;lt;- c(metric, paste0(metric, &amp;quot;_lo&amp;quot;), paste0(metric, &amp;quot;_hi&amp;quot;))
  
  df %&amp;gt;% 
    ggplot(aes_string(x = &amp;quot;year&amp;quot;, y = metric, col = &amp;quot;g_whoregion&amp;quot;, 
                      fill = &amp;quot;g_whoregion&amp;quot;)) +
    geom_point(size = 1.3) +
    geom_ribbon(aes_string(ymin = metric_vars[2], ymax = metric_vars[3]), alpha = 0.3) +
    geom_line(size = 1.1) +
    scale_y_continuous(labels = comma) +
    scale_colour_viridis(discrete = TRUE, option = &amp;quot;E&amp;quot;) +
    scale_fill_viridis(discrete = TRUE, option = &amp;quot;E&amp;quot;) +
    labs(title = title, subtitle = subtitle, 
         x = &amp;quot;Year&amp;quot;, y = y_lab) +
    theme_ipsum() +
    theme(legend.position = &amp;quot;none&amp;quot;) +
    facet_wrap(~g_whoregion, scales = scales)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now quickly plot TB incidence rates by region, as well as mortality rates by region.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_inc_region &amp;lt;- tb_burden %&amp;gt;% 
  rate_region(metric = &amp;quot;e_inc_num&amp;quot;) %&amp;gt;% 
  plot_rate_region(metric = &amp;quot;e_inc_num&amp;quot;,
                   title = &amp;quot;Tuberculosis Incidence Rates&amp;quot;,
                   subtitle = &amp;quot;By WHO region: 2000 to 2016&amp;quot;,
                   scales = &amp;quot;free_y&amp;quot;,
                   y_lab = &amp;quot;Tuberculosis Incidence Rates (per 100,000 population)&amp;quot;)

plot_inc_region +
  labs(caption = &amp;quot;@seabbs Source: WHO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-02-27-estimate-case-fatality-rate_files/figure-html/plot-tb-inc-reg-1.png&#34; width=&#34;2310&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_mort_region &amp;lt;- tb_burden %&amp;gt;% 
  rate_region(metric = &amp;quot;e_mort_num&amp;quot;) %&amp;gt;% 
  plot_rate_region(metric = &amp;quot;e_mort_num&amp;quot;,
                   title = &amp;quot;Tuberculosis Mortality Rates&amp;quot;,
                   subtitle = &amp;quot;By WHO region: 2000 to 2016&amp;quot;,
                   scales = &amp;quot;free_y&amp;quot;,
                   y_lab = &amp;quot;Tuberculosis Mortality Rates (per 100,000 population)&amp;quot;)

plot_mort_region +
  labs(caption = &amp;quot;@seabbs Source: WHO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-02-27-estimate-case-fatality-rate_files/figure-html/plot-tb-mort-reg-1.png&#34; width=&#34;2310&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This gives an encouraging picture with both incidence rates and mortality rates declining in all regions. As we observed from the maps above incidence rates (and mortality rates) are highest in Africa, and South-East Asia. We also see that the rate of decline in incidence rates varies across regions, with South-East Asia and the Eastern Mediterranean experiencing the slowest decreases. Interestingly it also appears that mortality rates in some areas are decreasing more quickly than incidence rates, this is most notable for Europe. As mortality rates and incidence rates are highly correlated to explore in more depth we need to estimate the case fatality ratio for all years in the data.&lt;/p&gt;
&lt;p&gt;To validate our estimates we first extract the regional estimates for the case fatality ratio based on the WHO estimates, by taking the mean (and standard deviation) of case fatality ratios by country in each region. This gives an estimate for case fatality rates at the regional level as well as the between country variation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Summarise Case fatality rate by region - only availble for 2016
region_case_fat &amp;lt;- tb_burden %&amp;gt;% 
  filter(year %in% 2016) %&amp;gt;% 
  group_by(year, g_whoregion) %&amp;gt;% 
  summarise(mean = mean(cfr, na.rm = TRUE),
            sd = sd(cfr, na.rm = TRUE)) %&amp;gt;% 
  mutate(ll = mean - sd,
         lll = mean - 2*sd,
         hh = mean + sd,
         hhh = mean + 2 * sd)

region_case_fat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
## # Groups:   year [1]
##    year g_whoregion             mean     sd       ll      lll    hh   hhh
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1  2016 Africa                0.238  0.0899  0.148    0.0581  0.328 0.418
## 2  2016 Americas              0.132  0.133  -0.00116 -0.134   0.265 0.397
## 3  2016 Eastern Mediterranean 0.131  0.129   0.00173 -0.127   0.260 0.389
## 4  2016 Europe                0.0837 0.0458  0.0379  -0.00792 0.130 0.175
## 5  2016 South-East Asia       0.135  0.0439  0.0916   0.0477  0.179 0.223
## 6  2016 Western Pacific       0.0885 0.0483  0.0402  -0.00810 0.137 0.185&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then estimate the regional case fatality ratio by taking the regional mortality rate and dividing it by the regional incidence rate for each year. We also do this for the high and low mortality rate estimates (using the lowest estimate for TB mortality and the highest estimate for incidence rates to get the lower bound, and vice versa for the high bound). This gives,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;region_case_fatality &amp;lt;- tb_burden %&amp;gt;% 
  rate_region(metric = &amp;quot;e_inc_num&amp;quot;) %&amp;gt;% 
  left_join(tb_burden %&amp;gt;% 
              rate_region(metric = &amp;quot;e_mort_num&amp;quot;)) %&amp;gt;% 
  mutate(case_fat_rate = e_mort_num / e_inc_num,
         case_fat_rate_lo = e_mort_num_lo / e_inc_num_hi,
         case_fat_rate_hi = e_mort_num_hi / e_inc_num_lo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;year&amp;quot;, &amp;quot;g_whoregion&amp;quot;, &amp;quot;e_pop_num&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;region_case_fatality&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 102 x 12
##     year g_whoregion        e_inc_num e_inc_num_lo e_inc_num_hi  e_pop_num
##    &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1  2000 Africa                 332.         210.         481.      6.59e8
##  2  2000 Americas                36.5         28.6         45.6     8.38e8
##  3  2000 Eastern Mediterra…     127.          87.0        175.      4.81e8
##  4  2000 Europe                  52.3         38.1         68.8     8.69e8
##  5  2000 South-East Asia        305.         175.         473.      1.57e9
##  6  2000 Western Pacific        131.          91.9        178.      1.70e9
##  7  2001 Africa                 337.         215.         487.      6.76e8
##  8  2001 Americas                35.0         27.4         43.6     8.48e8
##  9  2001 Eastern Mediterra…     126.          86.2        174.      4.92e8
## 10  2001 Europe                  51.9         37.7         68.3     8.70e8
## # ... with 92 more rows, and 6 more variables: e_mort_num &amp;lt;dbl&amp;gt;,
## #   e_mort_num_lo &amp;lt;dbl&amp;gt;, e_mort_num_hi &amp;lt;dbl&amp;gt;, case_fat_rate &amp;lt;dbl&amp;gt;,
## #   case_fat_rate_lo &amp;lt;dbl&amp;gt;, case_fat_rate_hi &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then plot this using the &lt;code&gt;plot_rate_region&lt;/code&gt; function outlined above, combined with addition layers to compare our estimate to that produced by the WHO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_region_case_fatality &amp;lt;- region_case_fatality %&amp;gt;%
  plot_rate_region(metric = &amp;quot;case_fat_rate&amp;quot;,
                   title = &amp;quot;Tuberculosis Case Fatality Rate&amp;quot;,
                   subtitle = &amp;quot;By WHO region: 2000 to 2016&amp;quot;,
                   scales = &amp;quot;free_y&amp;quot;,
                   y_lab = &amp;quot;Estimated TB Case Fatality Ratio&amp;quot;) +
  labs(caption = &amp;quot;Case fatality ratio estimated by taking the ratio of TB mortality rates and TB incidence rates each year in all years. For 2016 
       the mean regional case fatality ratio estimated by the WHO is also shown (along with one and two standard deviations)&amp;quot;) +
  geom_point(data = region_case_fat, aes(y = mean, x = year, fill = g_whoregion), shape = 2, size = 1.3, col = &amp;quot;black&amp;quot;) +
  geom_linerange(data = region_case_fat, aes(ymin = ll, ymax = hh, y = NULL), alpha = 0.4, size = 1.2, col = &amp;quot;black&amp;quot;) +
  geom_linerange(data = region_case_fat, aes(ymin = lll, ymax = hhh, y = NULL), alpha = 0.2, size = 1.2, col = &amp;quot;black&amp;quot;)
  
plot_region_case_fatality +
  labs(caption = &amp;quot;@seabbs Source: WHO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-02-27-estimate-case-fatality-rate_files/figure-html/plot-regional-cfr-1.png&#34; width=&#34;2310&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the estimate using mortality rates / incidence rates compares well to the estimate published for 2016 by the WHO, although there is substantial within country variation. For all regions our estimate has substantial uncertainty. The plot suggests that the TB case fatality ratio is decreasing over time in all regions, although this reduction appears to be very minimal in some regions (such as the Americas, Africa, and the Eastern Mediterranean). It is likely that this trend does not hold across all countries, but this is beyond the scope of this post (watch this space).&lt;/p&gt;
&lt;p&gt;As a first look at the case fatality ratios in individual countries we can quickly pull out the 10 countries with the highest case fatality ratio in 2016. These are,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;highest_case_fataltity_countries &amp;lt;- tb_burden %&amp;gt;% 
  filter(year %in% 2016) %&amp;gt;% 
  arrange(desc(cfr)) %&amp;gt;% 
  slice(1:10) %&amp;gt;% 
  pull(country)

highest_case_fataltity_countries&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Barbados&amp;quot;                    &amp;quot;United Arab Emirates&amp;quot;       
##  [3] &amp;quot;Anguilla&amp;quot;                    &amp;quot;Lesotho&amp;quot;                    
##  [5] &amp;quot;Ghana&amp;quot;                       &amp;quot;Guinea-Bissau&amp;quot;              
##  [7] &amp;quot;Nigeria&amp;quot;                     &amp;quot;United Republic of Tanzania&amp;quot;
##  [9] &amp;quot;Mozambique&amp;quot;                  &amp;quot;Kenya&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then plot the TB case fatality ratio in these countries, as well as the TB incidence rates over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot overview of cases fatality ratios
plot_high_cfr &amp;lt;- plot_tb_burden_overview(metric = &amp;quot;cfr&amp;quot;, 
                                         countries = highest_case_fataltity_countries,
                                         verbose = FALSE) +
  scale_color_viridis(option = &amp;quot;cividis&amp;quot;, direction = -1) +
  theme_ipsum() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;Tuberculosis Case Fatality Ratio - 2016&amp;quot;,
       subtitle = &amp;quot;For the countries with the 10 highest ratios&amp;quot;)

plot_high_cfr +
  labs(caption = &amp;quot;@seabbs Source: WHO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-02-27-estimate-case-fatality-rate_files/figure-html/plot-highest-cfr-countries-1.png&#34; width=&#34;2310&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_inc_high_cfr &amp;lt;- plot_tb_burden_overview(countries = highest_case_fataltity_countries,
                                             verbose = FALSE) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  scale_color_viridis(option = &amp;quot;cividis&amp;quot;, direction = -1) +
  theme_ipsum() +
  theme(legend.position = &amp;quot;bottom&amp;quot;) +
  labs(title = &amp;quot;Tuberculosis Incidence Rates - 2000:2016&amp;quot;,
       subtitle = &amp;quot;In the countries with the 10 highest TB case fatality ratios&amp;quot;)

plot_inc_high_cfr +
  labs(caption = &amp;quot;@seabbs Source: WHO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-02-27-estimate-case-fatality-rate_files/figure-html/plot-inc-rates-high-cfr-1.png&#34; width=&#34;2310&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that for some countries on this list (i.e Anguilla, the United Arab Emirates, and Barbados) the high case fatality ratio estimates are likely to be based on a very low sample size and therefore may not be trustworthy. Worryingly however for several countries (Lesotho, Tanzania, Kenya, Mozambique, Guinea-Bissau, and Ghana, and Nigeria) with extremely high case fatality ratios incidence rates are also very high, which will correspond to very high mortality rates. These countries are all in Africa, which as a region also had the highest incidence rates and the highest case fatality ratio. It is clear from these plots that Africa, and these countries in particular should be a particular focus of TB control efforts if TB mortality rates are to be reduced more quickly.&lt;/p&gt;
&lt;p&gt;Finally we summarise this post using &lt;code&gt;patchwork&lt;/code&gt;, a great new package by &lt;a href=&#34;https://twitter.com/thomasp85&#34;&gt;Thomas Pederson&lt;/a&gt;, which lets you easily combine ggplot plots into a single plot. The resulting storyboard (below) neatly summarises the exploratory plots, and the narrative they provide, that we have made in this post. See &lt;a href=&#34;https://www.samabbott.co.uk/post/2018-02-27-estimate-case-fatality-rate_files/figure-html/pathwork-storyboard-1.png&#34;&gt;here&lt;/a&gt; for a full size version.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;storyboard &amp;lt;- (mp1 | mp2 ) /
    {
      plot_region_case_fatality + {
        plot_high_cfr +
          plot_inc_high_cfr +
          labs(caption = &amp;quot;@seabbs Source: World Health Organisation&amp;quot;) +
          plot_layout(ncol = 1)
      } +
        plot_layout(ncol = 2, widths = c(3, 1))
    } +
  plot_layout(ncol = 1, heights = c(2, 4))

ggsave(&amp;quot;../../static/img/getTBinR/storyboard_tb_case_fatality_rate.png&amp;quot;,
       storyboard, width = 20, height = 15, dpi = 330)

storyboard&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-02-27-estimate-case-fatality-rate_files/figure-html/pathwork-storyboard-1.png&#34; width=&#34;7920&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The WHO TB data contains many more variables not mentioned in this post and getTBinR contains additional features, see the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;site for getTBinR&lt;/a&gt; for details. See &lt;a href=&#34;https://gist.github.com/seabbs&#34;&gt;my gists&lt;/a&gt; for code snippets to get you started in exploring the data further. Package feature requests are welcome, preferably by filing a &lt;a href=&#34;https://github.com/seabbs/getTBinR/issues&#34;&gt;GitHub issue&lt;/a&gt;, as are suggestions for additional data sources to include.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explore Global Tuberculosis</title>
      <link>http://www.samabbott.co.uk/project/exploreglobaltb/</link>
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/project/exploreglobaltb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BIDD Modelling Course</title>
      <link>http://www.samabbott.co.uk/project/biddmodellingcourse/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/project/biddmodellingcourse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Explore Infectious Disease Models</title>
      <link>http://www.samabbott.co.uk/project/exploreidmodels/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/project/exploreidmodels/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploring Global Trends in Tuberculosis Incidence Rates - with GetTBinR</title>
      <link>http://www.samabbott.co.uk/post/intro-gettbinr/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/post/intro-gettbinr/</guid>
      <description>


&lt;p&gt;In November I attended &lt;a href=&#34;https://www.elsevier.com/events/conferences/international-conference-on-infectious-disease-dynamics&#34;&gt;Epidemics&lt;/a&gt;, which is a conference focused on modelling infectious diseases. There was a lot of great work and perhaps most excitingly a lot of work being offered as R packages.&lt;/p&gt;
&lt;p&gt;I’ve recently begun wrapping all my analytical work in R packages, as it makes producing reproducible research a breeze! Unfortunately all of this work is still making it’s way towards publication and for a variety of reasons can’t be shared until it has passed this hurdle. As I didn’t want to be left out my only alternative was to produce an R package during my downtime at the conference - from which &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR&#34;&gt;getTBinR&lt;/a&gt; was born. Over the Christmas period more work has been done and the package is now available from &lt;a href=&#34;https://cran.r-project.org/web/packages/getTBinR/index.html&#34;&gt;CRAN&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;GetTBinR wraps the World Health Organisation &lt;a href=&#34;http://www.who.int/tb/country/data/download/en/&#34;&gt;Tuberculosis data&lt;/a&gt;, which includes data on Tuberculosis incidence rates, mortality rates, case reporting rates and several other metrics by country from 2000 until 2016 with varying levels of completeness. It also provides some general purpose functions for mapping and plotting the data, with the aim of kick starting any exploratory analysis of the WHO Tuberculosis data.&lt;/p&gt;
&lt;p&gt;For this post I will be quickly diving into global trends in Tuberculosis incidence rates and exploring whether Tuberculosis eradication is on the horizon.&lt;/p&gt;
&lt;p&gt;Now to get started, the first step is to get the package (for this post we are using the development version from GitHub) and to load the other packages required for this analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;seabbs/GetTBinR&amp;quot;)
library(getTBinR)
install.packages(&amp;quot;tidyverse&amp;quot;)
library(tidyverse)
install.packages(&amp;quot;viridis&amp;quot;)
library(viridis)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The package is loaded, time to get the data. We download both the data itself and it’s accompanying data dictionary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_burden &amp;lt;- get_tb_burden(download_data = TRUE, save = TRUE)
dict &amp;lt;- get_data_dict(download_data = TRUE, save = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to explore incidence rates so we need to find them in the data. We can do this using the &lt;code&gt;getTBinR::search_data_dict&lt;/code&gt; function,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;search_data_dict(dict = dict, def = &amp;quot;incidence&amp;quot;, verbose = FALSE) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;variable_name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;dataset&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;code_list&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;definition&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_100k&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence (all forms) per 100 000 population&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_100k_hi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence (all forms) per 100 000 population, high bound&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_100k_lo&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence (all forms) per 100 000 population, low bound&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_rr_num&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence of rifampicin resistant TB (absolute number)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_rr_num_hi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence of rifampicin resistant TB (absolute number): high bound&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_rr_num_lo&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence of rifampicin resistant TB (absolute number): low bound&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_tbhiv_100k&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence of TB cases who are HIV-positive per 100 000 population&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_tbhiv_100k_hi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence of TB cases who are HIV-positive per 100 000 population, high bound&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_tbhiv_100k_lo&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence of TB cases who are HIV-positive per 100 000 population, low bound&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_tbhiv_num&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence of TB cases who are HIV-positive&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_tbhiv_num_hi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence of TB cases who are HIV-positive, high bound&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;e_inc_tbhiv_num_lo&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimated incidence of TB cases who are HIV-positive, low bound&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The first hit, &lt;code&gt;e_inc_100k&lt;/code&gt;, is the TB incidence rate, with the next two variables being the lower and upper bounds of this estimate. For a quick overview lets map country specific incidence rates in 2016 using the getTBinR function &lt;code&gt;map_tb_burden&lt;/code&gt;,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_tb_burden(df = tb_burden, dict = dict,
              metric = &amp;quot;e_inc_100k&amp;quot;, verbose = FALSE) +
  labs(title = &amp;quot;Map of Global Tuberculosis Incidence Rates - 2016&amp;quot;,
       subtitle = &amp;quot;&amp;quot;, caption = &amp;quot;Source: World Health Organisation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-01-15-intro-tb-getTBinR_files/figure-html/map-tb-2016-inc-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This shows that incidence rates are highly heterogeneous between regions. To get an better understanding of this lets plot incidence rates by WHO region,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_inc_region &amp;lt;- tb_burden %&amp;gt;% 
  group_by(year, g_whoregion) %&amp;gt;% 
  summarise_at(.vars = vars(e_inc_num, e_inc_num_lo, e_inc_num_hi, e_pop_num),
               .funs = funs(sum(as.numeric(.), na.rm = T))) %&amp;gt;% 
  mutate_at(.vars = vars(e_inc_num, e_inc_num_lo, e_inc_num_hi),
            .funs = funs(inc_rate = . / e_pop_num * 1e5)) %&amp;gt;% 
  mutate(g_whoregion = ifelse(is.na(g_whoregion), &amp;quot;Asia&amp;quot;, g_whoregion))

plot_tb_inc_region &amp;lt;- function(df = NULL, title = NULL, subtitle = NULL, scales = NULL) {
  df %&amp;gt;% 
      ggplot(aes(x = year, y = e_inc_num_inc_rate, col = g_whoregion)) +
  geom_point() +
  geom_linerange(aes(ymin = e_inc_num_lo_inc_rate, ymax = e_inc_num_hi_inc_rate)) +
  geom_line() +
  scale_colour_viridis(discrete = TRUE) +
  labs(title = title, subtitle = subtitle, 
       x = &amp;quot;Year&amp;quot;, y = &amp;quot;Tuberculosis Incidence Rates (per 100,000 population)&amp;quot;,
       caption = &amp;quot;Source: World Health Organisation&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  facet_wrap(~g_whoregion, scales = scales)
}

tb_inc_region %&amp;gt;% 
  plot_tb_inc_region(title = &amp;quot;Global Tuberculosis Incidence Rates&amp;quot;,
                     subtitle = &amp;quot;By WHO region, with a fixed y axis&amp;quot;,
                     scales = &amp;quot;fixed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-01-15-intro-tb-getTBinR_files/figure-html/plot-region-tb-inc-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that incidence rates are much higher in Africa, and in Asia, than in other regions, and that incidence rates in the Americas and Europe are the lowest. This chart has a fixed y axis which makes it hard to see trends over time within regions, if we repeat it with a free y axis the trends over time become more apparent,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb_inc_region %&amp;gt;% 
  plot_tb_inc_region(title = &amp;quot;Global Tuberculosis Incidence Rates&amp;quot;,
                     subtitle = &amp;quot;By WHO region, with a variable y axis&amp;quot;,
                     scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-01-15-intro-tb-getTBinR_files/figure-html/plot-region-tb-inc-free-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows that Tuberculosis incidence rates are decreasing in all regions, which is a great sign for the elimination of Tuberculosis. However whilst this is true on the regional level it may not be true for all countries in the data set, something that is required to truly eradicate Tuberculosis. To explore this we find the countries that had higher incidence rates in 2016 than in 2000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;countries_inc_up &amp;lt;- tb_burden %&amp;gt;% 
  filter(year %in% c(2000, 2016)) %&amp;gt;% 
  group_by(country) %&amp;gt;% 
  arrange(desc(e_inc_100k)) %&amp;gt;% 
  slice(1) %&amp;gt;% 
  filter(year == 2016) %&amp;gt;% 
  pull(country)

high_inc_countries &amp;lt;- tb_burden %&amp;gt;% 
  filter(year == 2016) %&amp;gt;% 
  group_by(country) %&amp;gt;% 
  summarise(e_inc_100k = max(e_inc_100k)) %&amp;gt;% 
  ungroup %&amp;gt;% 
  arrange(desc(e_inc_100k)) %&amp;gt;% 
  slice(1:20) %&amp;gt;% 
  pull(country) %&amp;gt;% 
  unique

high_inc_up_countries &amp;lt;- intersect(countries_inc_up, high_inc_countries)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This results in a list of 44 countries all of which had higher incidence rates in 2016 than in 2000. Of these countries 8 were in the top 20 countries by incidence rate in 2016. This can plotted below using &lt;code&gt;getTBinR::plot_tb_burden_overview&lt;/code&gt;,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden_overview(df = tb_burden,
                        dict = dict, 
                        metric = &amp;quot;e_inc_100k&amp;quot;, 
                        countries = high_inc_up_countries,
                        verbose = FALSE) +
  labs(title = &amp;quot;Tuberculosis Incidence Rates from 2000-2016&amp;quot;,
       subtitle = &amp;quot;Showing countries with the highest incidence rates in which incidence rates are increasing&amp;quot;,
       caption = &amp;quot;Source: World Health Organisation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-01-15-intro-tb-getTBinR_files/figure-html/plot-tb-inc-high-inc-overview-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;GetTBinR also supplies another function (&lt;code&gt;plot_tb_burden&lt;/code&gt;) that can be used to visualise this,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden(df = tb_burden,
               dict = dict,
               metric = &amp;quot;e_inc_100k&amp;quot;, countries = high_inc_up_countries, 
               facet = &amp;quot;country&amp;quot;, scales = &amp;quot;free_y&amp;quot;, verbose = FALSE) +
  labs(title = &amp;quot;Tuberculosis Incidence Rates from 2000-2016&amp;quot;,
       subtitle = &amp;quot;Showing countries with the highest incidence rates in which incidence rates are increasing&amp;quot;,
       caption = &amp;quot;Source: World Health Organisation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-01-15-intro-tb-getTBinR_files/figure-html/plot-tb-inc-high-inc-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A possible cause of this may be that reporting of Tuberculosis notifications has improved over time, to understand this we first find the required variable in the data,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;search_data_dict(dict = dict, def = &amp;quot;detection&amp;quot;, verbose = FALSE) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;variable_name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;dataset&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;code_list&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;definition&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;c_cdr&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Case detection rate (all forms), percent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;c_cdr_hi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Case detection rate (all forms), percent, high bound&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;c_cdr_lo&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Estimates&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Case detection rate (all forms), percent, low bound&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We then plot the case detection rate over time in the countries of interest,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_tb_burden(df = tb_burden,
               dict = dict,
               metric = &amp;quot;c_cdr&amp;quot;, countries = high_inc_up_countries, 
               facet = &amp;quot;country&amp;quot;, scales = &amp;quot;free_y&amp;quot;, verbose = FALSE) +
  labs(title = &amp;quot;Tuberculosis Detection Rates from 2000-2016&amp;quot;,
       subtitle = &amp;quot;Showing countries with the highest incidence rates in which incidence rates are increasing&amp;quot;,
       caption = &amp;quot;Source: World Health Organisation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2018-01-15-intro-tb-getTBinR_files/figure-html/plot-tb-inc-high-case-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above shows that for some countries such as Mozambique and South Africa the increases in incidence rates may be linked to increases in the cases detection rate. However, it is clear that this is not the case for all countries, in particular the Congo which has seen increasing incidence rates and falling case detection rates.&lt;/p&gt;
&lt;p&gt;This post has highlighted the fact that whilst Tuberculosis is declining globally this is not the case in all countries. In order for the global eradication of Tuberculosis to be a success it is vital that resources are targeted at those countries which are struggling to reduce their incidence rates.&lt;/p&gt;
&lt;p&gt;The WHO Tuberculosis data contains many variables not mentioned in this post and has the potential to answer a number of other questions. See the &lt;a href=&#34;https://www.samabbott.co.uk/getTBinR/&#34;&gt;package site&lt;/a&gt; for additional package features not mentioned in this post. See &lt;a href=&#34;https://gist.github.com/seabbs&#34;&gt;my gists&lt;/a&gt; for code snippets to get you started. Feature requests are welcome, preferably by filing a &lt;a href=&#34;https://github.com/seabbs/getTBinR/issues&#34;&gt;GitHub issue&lt;/a&gt;, as are suggestions for additional data sources to include.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Show Me Perseus</title>
      <link>http://www.samabbott.co.uk/project/showmeperseus/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/project/showmeperseus/</guid>
      <description></description>
    </item>
    
    <item>
      <title>getTBinR</title>
      <link>http://www.samabbott.co.uk/project/gettbinr/</link>
      <pubDate>Thu, 30 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.samabbott.co.uk/project/gettbinr/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
