---
title: "Clustering Counties in England based on Tuberculosis Monitoring Indicators - using fingertipsR "
author: "null"
date: '2018-03-28'
description: "Using fingertipsR, principal component analysis, and partitioning around medoids to identify clusters of counties based on Tuberculosis monitoring indicators"
slug: cluster-england-tb
categories: ["R"]
tags: ["data analysis", "data visualisation", "rstats", "TB", "PHE", "infectious disease"]
draft: true
---

```{r knitr-opts, echo = FALSE}
knitr::opts_chunk$set(cache = TRUE, dpi = 330, 
                      fig.height = 8, fig.width = 8,
                      warning = FALSE)
```

Using fingertipsR, principal component analysis, and partitioning around medoids to identify clusters of counties based on Tuberculosis monitoring indicators.

Get packages for analysis.

```{r get-packages, message = FALSE}
if (!require(pacman)) install.packages("pacman"); library(pacman)
p_load("viridis")
p_load("broom")
p_load("knitr")
p_load("ggfortify")
p_load("purrr")
p_load("FactoMineR")
p_load("cluster")
p_load("scales")
p_load("fingertipsR")
p_load("tidyverse")
p_load("knitr")
p_load_gh("thomasp85/patchwork", dependencies = TRUE)
```


Use fingertipsR to find TB related profiles.

```{r get-tb-profiles}
profs <- profiles()

sel_profs <- profs[grepl("TB", profs$ProfileName),]

sel_profs
```

Find the indicators available for each profile.

```{r get-tb-indicators}
tb_inds <- indicators(ProfileID = sel_profs$ProfileID)

kable(tb_inds)
```

Get the data for each indicator as a list of tibbles. This results in 12 tibbles, with the first being empty (TB incidence rates).

```{r get-tb_indicator-tibbles}
tb_df <- tb_inds$IndicatorID %>% map(~fingertips_data(IndicatorID = .))
```

Explore 3 year average TB incidence rates, extracting data for counties. Recode the value variable as recent incidence rates, also pulling the overall incidence of cases. According to the [fingertips](https://fingertips.phe.org.uk/profile/tb-monitoring) website (which contains tools to interactively explore the data) local authorities and CCGs with fewer than 20 TB cases per year, have had all data for the indicators (apart from three-year average TB incidence) suppressed to avoid deductive disclosure. We can therefore filter out these counties now to avoid issues with missing data later. We can also adjust the time period to represent the final year for each rolling average.

```{r get-tb-inc-rates}
tb_inc <- tb_df[[2]] %>% 
  filter(AreaType %in% "County & UA") %>% 
  select(AreaName, Sex, Age, Timeperiod,
         rec_inc_rate = Value, rec_inc = Count) %>% 
  filter(rec_inc >= 20) %>% 
  mutate(Timeperiod = Timeperiod %>% 
           str_split(" - ") %>% 
           map_chr(first) %>% 
           as.numeric %>% 
           {. + 2} %>% 
           as.character) %>% 
  select(-rec_inc)
```

Looking through the other tibbles they all have the same structure - we can write a function using this knowledge to speed up data extraction.

```{r fun-extract-raw-data}
tb_df_extraction <- function(tb_df, var_name, area_type = "County & UA") {
  df <- tb_df %>% 
    filter(AreaType %in% area_type) %>% 
    select(AreaName, Sex, Age, Value, Timeperiod) %>% 
    rename_at(.vars = vars(Value), funs(paste0(var_name)))
  
  return(df)
}
```

We now extract data for all remaining indicators, rename variables with meaningful names, join into a single tibble and then left join onto the TB incidence rate tibble. Data is only available aggregated for all ages and genders so we also drop these variables here. Finally we clean up Timeperiod into years.

```{r make-com-tibble, messages = FALSE}
var_names <- c("prop_pul_cc", "prop_cc_ds_front", "prop_ds_treat_com_12",
               "prop_ds_lost_to_follow", "prop_ds_died",
               "prop_tb_offered_hiv_test", "prop_ds_rf_treat_com_12",
               "prop_cc_dr_front", "prop_p_start_treat_2_m_sym",
               "prop_p_start_treat_4_m_sym"
)

extracted_tb <- map2(tb_df[-(1:2)], var_names, ~tb_df_extraction(.x, .y)) %>% 
  reduce(full_join)

com_tb_df <- tb_inc %>% 
  left_join(extracted_tb) %>% 
  mutate(year = Timeperiod %>% as.numeric) %>% 
  mutate_if(is.character, as.factor) %>% 
  select(-Timeperiod) %>% 
  filter(Sex %in% "Persons", Age %in% "All ages") %>% 
  select(-Sex, -Age)

com_tb_df
```

We now need to check the completeness of the data. Ideally we would use the most recent year of data for our clustering analysis but this may not be possible if variables are highly missing.

```{r explore-missing-data-per-year}
get_frac_missing <- function(df) {
  df %>% 
    nest() %>% 
    mutate(missing = map(data,~map_dfr(. ,~sum(is.na(.))/length(.)))) %>% 
    select(-data) %>% 
    unnest(missing) 
}

## Get the proportion missing per variableby year
tb_miss_per_year <- com_tb_df %>% 
  group_by(year) %>% 
  get_frac_missing %>% 
  arrange(year) 

kable(tb_miss_per_year)
```

We see that data completeness increases with time but that some variables are completely missing (`prop_ds_rf_treat_com_12` and	`prop_cc_dr_front`). We therefore drop these variables and then identify which year has the lowest amount of missing data across all variables (by looking at mean missingness).

```{r drop-miss-explore-year-miss}
## Drop full missing variables
tb_partial_miss_year <- tb_miss_per_year %>% 
  select_if(~!sum(.) == length(.))

## Full missing variables
com_miss_vars <- setdiff(names(tb_miss_per_year), names(tb_partial_miss_year))

## Which year has the most complete data
tb_complete_years_all_vars <- com_tb_df %>% 
  group_by(year) %>% 
  nest() %>% 
  mutate(missing = map(data,~mean(colSums(is.na(.))/nrow(.)))) %>% 
  select(-data) %>% 
  unnest(missing) %>% 
  arrange(year)

kable(tb_complete_years_all_vars)
```


The above table indicates that 2016 has a high proportion of missing data. From the previous table we saw that this was partially due some variables being completely missing. The next best option is 2015, this has a higher proportion of missing data to previous years, but has no variables that are completely missing and is the most relevant after 2016. The final question is to what extent missingness is still related to TB incidence rate, the following table investigates this by looking at what happens as counties are excluded using varying incidence rate cut-offs.

```{r miss-df-miss-per-year, message = FALSE}
com_tb_df %>%
  filter(year == 2015) %>% 
  select(-map_dbl(com_miss_vars, contains)) %>% 
  mutate(inc_rate_lower = list(seq(2, 20, 2))) %>% 
  unnest(inc_rate_lower) %>% 
  group_by(year, inc_rate_lower) %>% 
  filter(rec_inc_rate > inc_rate_lower) %>% 
  nest() %>% 
  mutate(missing = map(data,~mean(colSums(is.na(.))/nrow(.)))) %>% 
  select(-data, -year) %>% 
  unnest(missing) %>% 
  kable
```

The choice of incidence rate cut-off is somewhat arbitary. However, it appears that a cut-off of at least 10 (per 100,000) is sufficient to deal with the majority of missing data. This is also a sensible cut-off as it represents the World Health Organisation's 2035 target for TB eradication. This means that our analysis will focus on counties that have relatively high incidence rates in comparision to the median in England. Using everything we have learnt about about the qaulity of the data we now identify the near final analysis dataset.

```{r get-analysis-df, message = FALSE}
tb_df_2015 <- com_tb_df %>% 
  select(-map_dbl(com_miss_vars, contains)) %>% 
  filter(year == 2015) %>% 
  filter(rec_inc_rate > 10)

tb_df_2015 
```

The final step is to deal with the remaining missing data. As all variables except incidence rate is missing we cannot reliably impute the data, we therefore drop it and make a note of the counties for which data was not avialable.

```{r final-analysis-df}
tb_clean_2015 <- tb_df_2015 %>% 
  drop_na() %>% 
  select(-year)

missing_regions <- setdiff(tb_df_2015$AreaName %>% as.character, tb_clean_2015$AreaName %>% as.character)

missing_regions
```

We are now ready to conduct some clustering analysis on this data. The first step is to reduce the dimensionality of the data using principal component analysis (PCA). We use the `estim_ncp` function (which uses a method outlined in [paper](https://www.sciencedirect.com/science/article/pii/S0167947311004099)) from the `FactoMineR` package to estimate the number of principal components required, perform PCA, and the plot the variance explained by each component as a check on `estim_ncp`. All of the following analysis is done using nested tibbles and so can be easily generalised to higher dimensional use cases. 

```{r perform-pca}
tb_pca <- tb_clean_2015 %>% 
  nest() %>% 
  mutate(
    numeric_data = map(data, ~select_if(., is.numeric) %>% 
                         as.data.frame()),
    optimal_pca_no = map(numeric_data, ~estim_ncp(., 
                                                  scale = TRUE, 
                                                  ncp.min = 2, 
                                                  ncp.max = 10)) %>% 
      map_dbl(~.$ncp),
    pca = map(numeric_data, ~prcomp(.x, 
                                    center = TRUE, 
                                    scale = TRUE)),
    pca_data = map(pca, ~.$x),
    pca_aug = map2(pca, data, ~augment(.x, data = .y)))
```

We find that the optimal number of principal components is `tb_pca$optimal_pca_no`. We can also plot the percentage of variance explained in order to evaluate this choice.

```{r extract-var-explained}
## Variance explained
var_exp <- tb_pca %>% 
  select(-optimal_pca_no) %>% 
  unnest(pca_aug) %>% 
  summarize_at(.vars = vars(contains("PC")), .funs = funs(var)) %>% 
  gather(key = pc, value = variance) %>% 
  mutate(var_exp = variance/sum(variance) * 100,
         cum_var_exp = cumsum(var_exp),
         pc = str_replace(pc, ".fitted", "") %>% 
           str_replace("PC", ""))
```

```{r plot-var-explained, fig.height = 8, fig.width = 8, dpi = 330}
var_exp %>% 
  rename(
    `Variance Explained` = var_exp,
    `Cumulative Variance Explained` = cum_var_exp
  ) %>% 
  gather(key = key, value = value, `Variance Explained`, `Cumulative Variance Explained`) %>%
  mutate(key = key %>% 
           factor(levels  = c("Variance Explained", 
                              "Cumulative Variance Explained"))) %>% 
  mutate(value = value / 100) %>% 
  mutate(pc = factor(pc, levels = as.character(1:max(var_exp$pc %>% as.numeric)))) %>% 
  ggplot(aes(pc, value, group = key)) + 
  geom_point(size = 2, alpha = 0.8) + 
  geom_line(size = 1.1, alpha = 0.6) + 
  facet_wrap(~key, scales = "free_y") +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 1, 0.05), lim = c(0, NA),
                     minor_breaks = NULL, labels = percent) +
  labs(
    title = "Variance Explained by Principal Component",
    subtitle = paste0("The optimal number of principal components suggested by estim_ncp was ",
                      tb_pca$optimal_pca_no, " which explains ", round(var_exp$cum_var_exp[[2]], 0), "% of the data."),
    x = "Principal Component",
    y = "Variance Explained (%)",
    caption = "@seabbs Source: Public Health England (fingertipsR)"
  )
```

The above plot shows that only `r paste0(round(var_exp$cum_var_exp[[2]], 0), "%")` of the variance in the data is explained by the first two principle components even though the `estim_ncp` function suggested that this was the optimal number. This indicates that there is large amount of noise in the data, with a large amount of non-systematic between county variation.

We can now perform clustering using the partitioning around medoids algorithm on the first two principal components, this approach should be more stable than K means and also has the benefit of producing a metric (the average silhouette width) which can be used to estimate the number of clusters which provides the best fitting model. Again we use an approach that makes use of nested tibbles, this should be easier to generalise to other use cases.

```{r pam-on-tb}
## Perform pam on pca data 1 to 10 groups
tb_pca_pam <- tb_pca %>%
  mutate(centers = list(2:10)) %>% 
  unnest(centers, .preserve = everything()) %>% 
  select(-centers, centers = centers1) %>% 
  group_by(centers) %>% 
  mutate(
    pam = map(pca_data,
              ~ pam(x = .x[, 1:optimal_pca_no], k = centers, stand = TRUE)),
    clusters = map(pam, ~.$clustering),
    avg_silhouette_width = map(pam, ~.$silinfo$avg.width),
    data_with_clusters = map2(.x = data, .y = clusters, ~mutate(.x, cluster = factor(.y, ordered = TRUE)))
  ) %>% 
  ungroup

tb_pca_pam
```

To asssess the optimal number of clusters we can plot the average silhouette width. This indicates that two clusters are optimal, although this estimate may not be that robust as the average silhouette width is low (0.38), with 6, 7, 8 clusters also having average silhouette widths that are comparable. In general however we should prefer the parsimonous model and therefore we will only investigate two clusters for the remainder of this post.

```{r avg-silhouette, fig.height = 8, fig.weight = 8, dpi = 330}
## Get max silhouette width
max_silhouette_width <- tb_pca_pam %>% 
  select(centers, avg_silhouette_width) %>% 
  unnest(avg_silhouette_width) %>% 
  arrange(desc(avg_silhouette_width)) %>% 
  slice(1)
  
## Plot average silhouette width
tb_pca_pam %>% 
  select(centers, avg_silhouette_width) %>% 
  unnest(avg_silhouette_width) %>% 
  ggplot(aes(x = centers, y = avg_silhouette_width)) +
  geom_line(size = 1.1, alpha = 0.6) +
  geom_point(size = 2, alpha = 0.8) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(1, 10, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(0, NA), breaks = seq(0, 1, 0.05), minor_breaks = NULL) +
  labs(title = "Average Silhouette Width by Number of PAM Clusters",
       subtitle = paste0("The optimal number of clusters identifed by avg. silhouette width was ",
                      max_silhouette_width$centers,
                      " with an avg. silhouette width of ", 
                      round(max_silhouette_width$avg_silhouette_width, 2)
       ),
       x = "Clusters",
       y = "Avg. Silhouette Width",
       caption = "@seabbs Source: Public Health England (fingertipsR)")
```

We can now explore the clusters we have identified. A useful way to do this is to visual the first two principal components, overlaid with the original variable loadings, and the clusters we have identified.

```{r plot-pca, fig.height = 8, fig.width = 8, dpi = 330}
## Plot clusters
pca_plot <- tb_pca_pam %>% 
  filter(centers == max_silhouette_width$centers) %>% 
  select(data_with_clusters, pca) %>% 
  mutate(pca_graph = map2(.x = pca, 
                          .y = data_with_clusters,
                          ~ autoplot(.x, x = 1, y = 2, loadings = TRUE, loadings.label = TRUE,
                                     loadings.label.repel = TRUE, loadings.label.size = 2, loadings.alpha = 0.8,
                                     loadings.label.vjust = -1,
                                     data = .y, label = TRUE, label.label = "AreaName", label.size = 1.5,
                                     label.vjust = -1, alpha = 0.3, frame = TRUE, frame.type = 'convex',
                                     frame.alpha= 0.05,
                                     colour = "cluster", size = "rec_inc_rate") +
                            theme_minimal() +
                            labs(x = paste0("Principal Component 1 (Variance Explained: ",
                                            round(var_exp$var_exp[[1]], 1), "%)"),
                                 y = paste0("Principal Component 2 (Variance Explained: ",
                                            round(var_exp$var_exp[[2]], 1), "%)")) +
                            guides(colour=guide_legend(title = "Cluster", ncol = 2), 
                                   fill=guide_legend(title= "Cluster", ncol = 2),
                                   size = guide_legend(title = "TB Incidence Rate (per 100,000 population)",
                                                       ncol = 2)) +
                            scale_colour_viridis(option = "viridis", discrete = TRUE, end = 0.5) +
                            scale_fill_viridis(option = "viridis", discrete = TRUE, end = 0.5) +
                            theme(legend.position = "bottom", legend.box = "horizontal") +
                            labs(
                              title = "Tuberculosis Data in England; First Two Principal Components",
                              subtitle = "The arrows are variable loadings and points are counties coloured by cluster membership",
                              caption = "@seabbs Source: Public Health England (fingertipsR)"
                            )
  )) %>% 
  pull(pca_graph) %>% 
  first


pca_plot
```

From this we see that the clusters are generally split by incidence rates with lower incidence rate counties also having a higher proportion of cases die and a higher proportion of cases lost to follow up. The higher incidence counties have a higher proportion of cases that are pulmonary, and more cases that complete treatment within 6 months. It appears that the proportion of cases that start treatment within 2 and 4 months varies over both clusters. Another way of summarising the between cluster differences is to summarise the data by cluster, which is presented in the following plot.


```{r summary-plot, fig.height = 8, fig.width = 8, dpi = 330}
sum_tb_df <- tb_pca_pam %>% 
  filter(centers == max_silhouette_width$centers) %>% 
  pull(data_with_clusters) %>% 
  map(~ gather(., key = "Variable", value = "value", -AreaName, -cluster)) %>% 
  first %>% 
  rename(Cluster = cluster) 

sum_tb_df %>% 
  ggplot(aes(x = Variable, y = value, col = Cluster, fill = Cluster)) +
  geom_violin(draw_quantiles = c(0.025, 0.5, 0.975), alpha = 0.2, scale = "width") +
  geom_jitter(position = position_jitterdodge(), alpha = 0.3) +
  coord_flip() +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 100, 5), minor_breaks = NULL) +
  scale_colour_viridis(option = "viridis", discrete = TRUE, end = 0.5) +
  scale_fill_viridis(option = "viridis", discrete = TRUE, end = 0.5) +
  labs( 
    title = "Tuberculosis in England; Summarised by Cluster",
    subtitle = "Violin plots are scaled by width, with the 2.5%, 50% and 97.5% quantiles shown.",
    x = "Variable",
    y = "Incidence rate (per 100,000) for rec_int_rate, otherwise proportion (0-100%)",
    caption = "@seabbs Source: Public Health England (fingertipsR)")
```

To explore this further we can calculate the percentage difference between clusters for each several summary statistics, for each variable. The following table does this for the mean, the median, the 2.5% quantile, and the 97.5% quantile.

```{r diff-tab}
sum_tb_df %>% 
  group_by(Cluster, Variable) %>% 
  summarise(mean = mean(value), median = median(value),
            lll = quantile(value, 0.025),
            hhh = quantile(value, 0.975)) %>% 
  group_by(Variable) %>% 
  mutate_if(is.numeric, .funs = funs((lag(.) - .)/ .)) %>% 
  drop_na %>% 
  mutate_if(is.numeric, .funs = funs(paste0(round(. * 100, 1), "%"))) %>% 
  ungroup %>% 
  mutate(Variable = factor(Variable, levels = rev(.$Variable))) %>% 
  arrange(Variable) %>% 
  rename(Mean = mean, Median = median, `2.5% Quantile` = lll, `97.5% Quantile` = hhh) %>% 
  select(-Cluster) %>% 
  kable
```

A final way to understand the clustering of Tuberculosis in England based on the data we have extracted using the `fingertipsR` package is to plot the cluster membership for each country on a map.

```{r map-cluster-membership}
tb_pca_pam %>% 
  filter(centers == max_silhouette_width$centers) %>% 
  pull(data_with_clusters)


```